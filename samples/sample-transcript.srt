1
00:00:05,030 --> 00:00:07,560
Okay. So let's, uh,

2
00:00:07,560 --> 00:00:10,290
get started with the actual, ah, technical content.

3
00:00:10,290 --> 00:00:12,720
So remember from last time,

4
00:00:12,720 --> 00:00:14,790
we gave an overview of the class.

5
00:00:14,790 --> 00:00:19,170
We talked about different types of models that we're gonna explore: reflex models,

6
00:00:19,170 --> 00:00:20,670
state-based models, variable based models,

7
00:00:20,670 --> 00:00:23,295
and logic models which we'll see throughout the course.

8
00:00:23,295 --> 00:00:26,625
But underlying all of this is, is, you know, machine learning.

9
00:00:26,625 --> 00:00:30,705
Because machine learning is what allows you to take data and, um,

10
00:00:30,705 --> 00:00:32,100
tune the parameters of the model,

11
00:00:32,100 --> 00:00:35,370
so you don't have to, ah, work as hard designing the model.

12
00:00:35,370 --> 00:00:37,830
Um, so in this lecture,

13
00:00:37,830 --> 00:00:40,530
I'm gonna start with the simplest of the models,

14
00:00:40,530 --> 00:00:42,510
the reflex based models, um,

15
00:00:42,510 --> 00:00:45,950
and show how machine learning can be applied to these type of models.

16
00:00:45,950 --> 00:00:47,600
And throughout the class, ah,

17
00:00:47,600 --> 00:00:50,060
we're going to talk about different types of models

18
00:00:50,060 --> 00:00:53,005
and how learning will help with those as well.

19
00:00:53,005 --> 00:00:55,020
So there's gonna be three parts,

20
00:00:55,020 --> 00:00:56,960
we're gonna talk about linear predictors, um,

21
00:00:56,960 --> 00:00:59,510
which includes classification regression, um,

22
00:00:59,510 --> 00:01:03,470
loss minimization which is basically stating an objective function of how you, ah,

23
00:01:03,470 --> 00:01:05,420
want to train your machine learning model,

24
00:01:05,420 --> 00:01:07,280
and then stochastic gradient descent,

25
00:01:07,280 --> 00:01:09,680
which is an algorithm that allows you to actually,

26
00:01:09,680 --> 00:01:11,425
ah, do the work.

27
00:01:11,425 --> 00:01:14,385
So let's start with, ah, perhaps the most,

28
00:01:14,385 --> 00:01:16,470
um, cliched example of,

29
00:01:16,470 --> 00:01:17,685
uh, you know, machine learning.

30
00:01:17,685 --> 00:01:21,350
So you have- we wanted to do spam classification.

31
00:01:21,350 --> 00:01:23,335
So the input is x,

32
00:01:23,335 --> 00:01:25,035
um, an email message.

33
00:01:25,035 --> 00:01:29,660
Um, and you wanna know whether an email message is spam or not spam.

34
00:01:29,660 --> 00:01:35,060
Um, so we're gonna denote the output of the classifier to be Y which is in this case,

35
00:01:35,060 --> 00:01:36,770
either spam or not spam.

36
00:01:36,770 --> 00:01:38,655
And our goal is to, ah,

37
00:01:38,655 --> 00:01:41,460
produce a predictor F, right?

38
00:01:41,460 --> 00:01:44,420
So a predictor in general is going to be a-

39
00:01:44,420 --> 00:01:47,705
a function that maps some input x to some output y.

40
00:01:47,705 --> 00:01:50,390
In this case, it's gonna take an email message and

41
00:01:50,390 --> 00:01:53,960
map it to whether the email message is spam or not.

42
00:01:53,960 --> 00:01:58,055
Okay. So there- there's many types of prediction problems, um,

43
00:01:58,055 --> 00:02:03,775
binary classification is the simplest one where the output is one of two,

44
00:02:03,775 --> 00:02:06,945
um, possibilities either yes or no.

45
00:02:06,945 --> 00:02:10,400
And we're gonna usually denote this as plus 1 or minus 1,

46
00:02:10,400 --> 00:02:12,590
sometimes you'll also see 1 and 0.

47
00:02:12,590 --> 00:02:16,270
Um, there's regression where you're trying to predict a numerical value,

48
00:02:16,270 --> 00:02:18,680
for example, let's say housing price.

49
00:02:18,680 --> 00:02:23,170
Um, there's a multi-class classification where Y is, ah,

50
00:02:23,170 --> 00:02:27,015
not just two items but possibly, um, 100 items,

51
00:02:27,015 --> 00:02:28,665
maybe cat, dog, truck,

52
00:02:28,665 --> 00:02:31,235
tree, and different kind of image categories.

53
00:02:31,235 --> 00:02:34,255
Um, there's ranking where the output,

54
00:02:34,255 --> 00:02:37,180
um, is a permutation of the input, this can be useful.

55
00:02:37,180 --> 00:02:40,310
For example, if the input is a set of, um, articles,

56
00:02:40,310 --> 00:02:41,870
or products, or webpages,

57
00:02:41,870 --> 00:02:44,270
and you want to rank them in some order to show to a user.

58
00:02:44,270 --> 00:02:46,760
Um, structured prediction is where Y,

59
00:02:46,760 --> 00:02:50,240
ah, the output is an object that is much more complicated.

60
00:02:50,240 --> 00:02:53,510
Um, perhaps, it's a whole sentence or even an image.

61
00:02:53,510 --> 00:02:56,120
And it's something that you have to kind of construct,

62
00:02:56,120 --> 00:02:57,710
you have to build this thing from scratch,

63
00:02:57,710 --> 00:02:59,675
it's not just a labeling.

64
00:02:59,675 --> 00:03:02,930
Um, and there's many more types of prediction problems.

65
00:03:02,930 --> 00:03:05,520
Um, but underlying all of this,

66
00:03:05,520 --> 00:03:08,180
you know, whenever someone says I'm gonna do machine learning.

67
00:03:08,180 --> 00:03:10,295
The first question you should ask is, okay what's the data?

68
00:03:10,295 --> 00:03:12,730
Because without data, there's no learning.

69
00:03:12,730 --> 00:03:15,750
So we're gonna call an example.

70
00:03:15,750 --> 00:03:20,000
Um, x, y pair is something that specifies

71
00:03:20,000 --> 00:03:24,390
what the output should be when the input is x, okay?

72
00:03:24,390 --> 00:03:27,710
And a training data or a set of examples,

73
00:03:27,710 --> 00:03:32,990
the training set is going to be simply a list or a multiset of, er, examples.

74
00:03:32,990 --> 00:03:36,755
So you can think about this as a partial specification of behavior.

75
00:03:36,755 --> 00:03:38,480
So remember, we're trying to design a system

76
00:03:38,480 --> 00:03:40,520
that has certain- certain types of behaviors,

77
00:03:40,520 --> 00:03:43,910
and we're gonna show you examples of what that sum should do.

78
00:03:43,910 --> 00:03:46,150
If I have some email message that has CS221

79
00:03:46,150 --> 00:03:48,800
then it's not spam but if it has,

80
00:03:48,800 --> 00:03:51,310
um, lots of, ah, dollar signs then there might,

81
00:03:51,310 --> 00:03:53,235
um, um, be spam.

82
00:03:53,235 --> 00:03:58,095
Um, and, ah- so remember this is not a false specification behavior.

83
00:03:58,095 --> 00:04:00,980
These, ah, ten examples or even a million examples

84
00:04:00,980 --> 00:04:03,800
might not tell you what exactly this function is supposed to do.

85
00:04:03,800 --> 00:04:06,050
It's just examples of, ah,

86
00:04:06,050 --> 00:04:09,335
what the function could do on those particular examples.

87
00:04:09,335 --> 00:04:12,035
Okay. So once you have this data,

88
00:04:12,035 --> 00:04:15,170
so we're gonna use D_train to denote, ah, the data set.

89
00:04:15,170 --> 00:04:17,620
Remember, it's a set of input output pairs.

90
00:04:17,620 --> 00:04:20,340
Um, we're going to,

91
00:04:20,340 --> 00:04:23,990
ah, push this into a learning algorithm or a learner.

92
00:04:23,990 --> 00:04:25,940
And what is the learning algorithm is gonna produce?

93
00:04:25,940 --> 00:04:27,650
It's gonna produce a predictor.

94
00:04:27,650 --> 00:04:32,060
So predictors are F and the predictor remember is what?

95
00:04:32,060 --> 00:04:34,700
It's actually itself a function that, um,

96
00:04:34,700 --> 00:04:38,750
takes an input x and maps it to an output y.

97
00:04:38,750 --> 00:04:41,990
Okay? So there's kind of two levels here.

98
00:04:41,990 --> 00:04:44,615
And you can understand this in terms of the,

99
00:04:44,615 --> 00:04:47,255
uh, modeling inferences of a learning paradigm.

100
00:04:47,255 --> 00:04:50,330
So modeling is about the question of what

101
00:04:50,330 --> 00:04:53,800
should the types of predictors after you should consider are.

102
00:04:53,800 --> 00:04:58,470
Ah, inference is about how do you compute y given x?

103
00:04:58,470 --> 00:05:00,560
And learning is about how you take

104
00:05:00,560 --> 00:05:04,880
data and produce a predictor so that you can do inference?

105
00:05:04,880 --> 00:05:07,820
Okay. Any questions about this so far?

106
00:05:07,820 --> 00:05:13,590
[NOISE]

107
00:05:13,590 --> 00:05:16,820
So this is pretty high level and abstract and generic right now,

108
00:05:16,820 --> 00:05:20,520
and this is kinda, kind of on purpose because I wanna highlight how, um,

109
00:05:20,520 --> 00:05:24,565
general machine learning is before going into the specifics of,

110
00:05:24,565 --> 00:05:26,355
uh, linear predictors, right?

111
00:05:26,355 --> 00:05:29,010
So this is an abstract framework.

112
00:05:29,500 --> 00:05:33,799
Okay. So let's dig in a little bit to this actual,

113
00:05:33,799 --> 00:05:35,900
um, an actual problem.

114
00:05:35,900 --> 00:05:38,580
Um, so just to simplify,

115
00:05:38,580 --> 00:05:40,575
ah, the email problem,

116
00:05:40,575 --> 00:05:43,200
let's, eh, consider the task of, um,

117
00:05:43,200 --> 00:05:46,445
predicting whether a string is an email address or not.

118
00:05:46,445 --> 00:05:49,340
Okay. Um, so the input is an em-,

119
00:05:49,340 --> 00:05:51,840
ah, is a string and, ah,

120
00:05:51,840 --> 00:05:54,660
the output is- it's a binary classification problem,

121
00:05:54,660 --> 00:05:58,820
it's either 1 if it's an email or minus 1 if it's not, that's what you want.

122
00:05:58,820 --> 00:06:03,125
Um, um, so the first step of,

123
00:06:03,125 --> 00:06:05,625
um, doing linear prediction is,

124
00:06:05,625 --> 00:06:07,325
um, known as feature extraction.

125
00:06:07,325 --> 00:06:09,890
And the question you should ask yourself is,

126
00:06:09,890 --> 00:06:15,200
what properties of the input x might be relevant for predicting the output y?

127
00:06:15,200 --> 00:06:18,920
Right, so I say, I really highlighted might be, right?

128
00:06:18,920 --> 00:06:23,480
At this point, you're not trying to encode the actual set of rules that solves a problem,

129
00:06:23,480 --> 00:06:24,680
that would involve no learning,

130
00:06:24,680 --> 00:06:27,020
and that would just be trying to do it directly.

131
00:06:27,020 --> 00:06:30,050
But instead of- for learning you're kind of taking a,

132
00:06:30,050 --> 00:06:32,790
um, you know, a more of a backseat and you're saying,

133
00:06:32,790 --> 00:06:35,135
"Well, here are some hints that could help you."

134
00:06:35,135 --> 00:06:38,930
Okay. Ah, so formally,

135
00:06:38,930 --> 00:06:43,760
a feature extractor takes an input and outputs a set of feature name,

136
00:06:43,760 --> 00:06:45,900
feature value pairs, right?

137
00:06:45,900 --> 00:06:47,480
So I'll go through an example here.

138
00:06:47,480 --> 00:06:50,030
So if I have abc@gmail.com,

139
00:06:50,030 --> 00:06:52,790
what are the properties that might be useful for determining

140
00:06:52,790 --> 00:06:56,420
whether a string is an email address or not?

141
00:06:56,420 --> 00:06:59,575
Well, you might consider the length of the string,

142
00:06:59,575 --> 00:07:01,490
if it's greater than 10,

143
00:07:01,490 --> 00:07:05,330
maybe long strings are less likely to be email addresses than shorter ones.

144
00:07:05,330 --> 00:07:08,480
Um, and here, the feature name is length greater than 10.

145
00:07:08,480 --> 00:07:10,790
So that's just kind of a label of that feature,

146
00:07:10,790 --> 00:07:13,405
and the value of that feature is 1,

147
00:07:13,405 --> 00:07:14,970
ah, representing it's true.

148
00:07:14,970 --> 00:07:17,100
So it will be 0, if it's false.

149
00:07:17,100 --> 00:07:20,760
Here's another feature, the fraction of alphanumeric characters, right?

150
00:07:20,760 --> 00:07:23,760
So that happens to be 0.85 which is the number.

151
00:07:23,760 --> 00:07:28,195
Um, there might be features that test for a particular,

152
00:07:28,195 --> 00:07:30,320
um, you know, letters for example,

153
00:07:30,320 --> 00:07:34,130
that it doesn't contain an "at" sign or that has a, you know,

154
00:07:34,130 --> 00:07:36,875
feature value of 1 because there is an "at" sign,

155
00:07:36,875 --> 00:07:42,400
endsWith.com is one, endsWith.org is a 0 because that's not true.

156
00:07:42,400 --> 00:07:45,150
So, um, and there you could have many,

157
00:07:45,150 --> 00:07:46,680
many more features, ah,

158
00:07:46,680 --> 00:07:48,995
and we'll talk more about features on next time.

159
00:07:48,995 --> 00:07:52,880
But the point is that you have a set of properties,

160
00:07:52,880 --> 00:07:56,760
you're kind of distilling down this input which is could be a string,

161
00:07:56,760 --> 00:08:00,830
or could be an image, or could be something more complicated into kind of a

162
00:08:00,830 --> 00:08:03,440
um, you know, ground-up fashion that later,

163
00:08:03,440 --> 00:08:06,670
we'll see how a machine learning algorithm can take advantage of.

164
00:08:06,670 --> 00:08:08,805
Okay. So you have this, ah,

165
00:08:08,805 --> 00:08:12,440
feature vector which has- is a list of

166
00:08:12,440 --> 00:08:17,120
feature values and their associated names or labels.

167
00:08:17,120 --> 00:08:22,910
Okay. But later, we'll see that the- the names don't matter to the learning algorithm.

168
00:08:22,910 --> 00:08:25,100
So actually, what you should also think about

169
00:08:25,100 --> 00:08:28,115
the feature vector is simply a list of numbers,

170
00:08:28,115 --> 00:08:31,365
and just kind of on the side make a note that all this, you know.

171
00:08:31,365 --> 00:08:34,865
position number three corresponds to contains "@" and so on.

172
00:08:34,865 --> 00:08:37,160
Right, so I've distilled the-

173
00:08:37,160 --> 00:08:44,205
the email address abc@gmail.com into the list of numbers 0- or 1,

174
00:08:44,205 --> 00:08:46,230
0.85, 1, 1, 0.

175
00:08:46,230 --> 00:08:48,515
Okay. So that's feature extraction.

176
00:08:48,515 --> 00:08:52,640
It's kind of distilling complex objects into lists of numbers which we'll

177
00:08:52,640 --> 00:08:58,190
see is what the kind of the lingua franca of these machine learning algorithms is.

178
00:08:58,190 --> 00:09:02,195
Okay. So I'm gonna write some concepts on a board.

179
00:09:02,195 --> 00:09:03,785
There's gonna be a bunch of, um,

180
00:09:03,785 --> 00:09:05,350
concepts I'm going to introduce,

181
00:09:05,350 --> 00:09:08,180
and I'll just keep them up on the board for reference.

182
00:09:08,180 --> 00:09:12,950
So feature vector is again an important notion and it's denoted Phi,

183
00:09:12,950 --> 00:09:16,010
um, of x on input.

184
00:09:16,010 --> 00:09:18,065
So Phi itself- sometimes, you think about it, er,

185
00:09:18,065 --> 00:09:21,830
you call it the feature map which takes an input and returns, um, a vector,

186
00:09:21,830 --> 00:09:24,800
and this notation means that returns in general,

187
00:09:24,800 --> 00:09:27,935
ah, d-dimensional vector, so a list of d numbers.

188
00:09:27,935 --> 00:09:33,290
And, um, the components of this feature vector we can write down as Phi_1,

189
00:09:33,290 --> 00:09:37,660
Phi_2, all the way to Phi_d of x.

190
00:09:38,000 --> 00:09:42,240
Okay. So this notation is,

191
00:09:42,240 --> 00:09:43,785
eh, you know, convenient, um,

192
00:09:43,785 --> 00:09:48,380
because we're gonna start shifting our focus from thinking about

193
00:09:48,380 --> 00:09:53,180
the features as properties of input to features as kind of mathematical objects.

194
00:09:53,180 --> 00:09:57,320
So in particular, Phi of x is a point in a high-dimensional space.

195
00:09:57,320 --> 00:09:59,010
So if you had two features,

196
00:09:59,010 --> 00:10:00,915
that would be a point in two-dimensional space,

197
00:10:00,915 --> 00:10:02,570
but in general, you might have a million features,

198
00:10:02,570 --> 00:10:04,250
so that's a feature, ah,

199
00:10:04,250 --> 00:10:06,230
it's a point enough, a hundred- ah,

200
00:10:06,230 --> 00:10:08,125
uh, million dimensional space.

201
00:10:08,125 --> 00:10:11,300
So, you know, it might be hard to think about that space, but well,

202
00:10:11,300 --> 00:10:12,740
we'll see how we can, you know,

203
00:10:12,740 --> 00:10:16,800
deal with that in a later in a, in a bit.

204
00:10:17,080 --> 00:10:20,315
Okay. So- so that's a feature vector,

205
00:10:20,315 --> 00:10:22,870
you take an input and return a list of numbers.

206
00:10:22,870 --> 00:10:25,710
Okay. Um, and now,

207
00:10:25,710 --> 00:10:28,430
the second piece is a weight vector.

208
00:10:28,430 --> 00:10:30,080
So let me write down a weight vector.

209
00:10:30,080 --> 00:10:33,470
[NOISE]

210
00:10:33,470 --> 00:10:36,880
So a weight vector is going to be noted W.

211
00:10:36,880 --> 00:10:39,670
Um, and this is also,

212
00:10:39,670 --> 00:10:41,110
uh, a list of D numbers.

213
00:10:41,110 --> 00:10:43,780
It's a point in a D-dimensional space

214
00:10:43,780 --> 00:10:46,825
but we're gonna interpret it differently, as we'll see later.

215
00:10:46,825 --> 00:10:50,260
Okay. So- so a way to think about a weight vector is that,

216
00:10:50,260 --> 00:10:51,730
for each feature J.

217
00:10:51,730 --> 00:10:55,535
So for example, frac of Alpha, um,

218
00:10:55,535 --> 00:10:57,700
we're gonna have a real number WJ,

219
00:10:57,700 --> 00:11:00,940
that represents the contribution of that feature to the prediction.

220
00:11:00,940 --> 00:11:03,505
So this contribution is 0.6.

221
00:11:03,505 --> 00:11:06,220
So what does this 0.6 mean?

222
00:11:06,220 --> 00:11:09,220
So, so the way to think about this is that you have

223
00:11:09,220 --> 00:11:14,905
your weight vector and you have a feature vector of a particular input,

224
00:11:14,905 --> 00:11:20,410
and you want- the score of, uh, your prediction

225
00:11:20,410 --> 00:11:26,230
is going to be, uh, the dot product between the weight vector and the feature vector.

226
00:11:26,230 --> 00:11:33,580
Okay. So um, that's written W dot a phi of X um, which is um,

227
00:11:33,580 --> 00:11:35,320
written out as basically,

228
00:11:35,320 --> 00:11:38,740
looking at all the features and multiplying the feature of the value

229
00:11:38,740 --> 00:11:42,580
times the weight of that feature and summing up all those numbers.

230
00:11:42,580 --> 00:11:43,960
So for this example,

231
00:11:43,960 --> 00:11:46,300
it will be minus 1.2,

232
00:11:46,300 --> 00:11:47,800
that's the weight of the first feature,

233
00:11:47,800 --> 00:11:50,410
times 1, that's the feature value,

234
00:11:50,410 --> 00:11:54,505
plus 0.6 times 0.85 and so on.

235
00:11:54,505 --> 00:11:58,300
And then, you get this number of 4.51 which is- happens to

236
00:11:58,300 --> 00:12:00,355
be the score for this example.

237
00:12:00,355 --> 00:12:02,380
Question?

238
00:12:02,380 --> 00:12:06,265
So the feature extraction which is phi of X, is that, uh,

239
00:12:06,265 --> 00:12:09,295
supposed to be like an automated process or is it a part of

240
00:12:09,295 --> 00:12:11,980
manual extraction classification procedures?

241
00:12:11,980 --> 00:12:13,315
Yeah. So the question is,

242
00:12:13,315 --> 00:12:16,360
is the feature extraction manual or automatic?

243
00:12:16,360 --> 00:12:24,130
So uh, phi is going to be implemented as a function like encode, right.

244
00:12:24,130 --> 00:12:28,210
Um, you're going to write this function manually.

245
00:12:28,210 --> 00:12:32,785
But you know, the function itself is run automatically on examples.

246
00:12:32,785 --> 00:12:36,010
Um, later we'll see how you can actually learn features as well.

247
00:12:36,010 --> 00:12:38,470
So you can slowly start to do less

248
00:12:38,470 --> 00:12:41,140
of a manual effort but uh, we're going to hold off until,

249
00:12:41,140 --> 00:12:41,950
next time for that.

250
00:12:41,950 --> 00:12:42,550
Question?

251
00:12:42,550 --> 00:12:44,260
So we're talking about weight gaining

252
00:12:44,260 --> 00:12:46,300
and I know that in certain tests of regressions like,

253
00:12:46,300 --> 00:12:49,750
uh, the weights being, uh, a percentage change,

254
00:12:49,750 --> 00:12:51,640
[inaudible] weights to percentage change of the

255
00:12:51,640 --> 00:12:53,875
outcome it doesn't, it doesn't mean the sphere?

256
00:12:53,875 --> 00:12:57,640
Yeah. So the question is about interpretation of weights.

257
00:12:57,640 --> 00:13:00,325
Sometimes weights can have a more precise meaning.

258
00:13:00,325 --> 00:13:03,940
In general, um, you can,

259
00:13:03,940 --> 00:13:08,710
you can try to read the tea leaves but it I don't think there is maybe, uh,

260
00:13:08,710 --> 00:13:11,890
in general a mathematically precise thing

261
00:13:11,890 --> 00:13:14,395
you can say about the meaning of individual weights.

262
00:13:14,395 --> 00:13:17,320
But intuitively, and the intuition is important,

263
00:13:17,320 --> 00:13:20,230
is that you should think about each feature as you know,

264
00:13:20,230 --> 00:13:23,380
a little person that's going to make a vote on this prediction, right?

265
00:13:23,380 --> 00:13:25,645
So you're voting either plus, yay or nay?

266
00:13:25,645 --> 00:13:29,785
And the weight of a particular feature is-

267
00:13:29,785 --> 00:13:34,045
specifies both the direction level whether- if positive weight means that,

268
00:13:34,045 --> 00:13:35,380
um, that little person, um,

269
00:13:35,380 --> 00:13:40,435
is voting positive and negative weight means, that it's voting negative.

270
00:13:40,435 --> 00:13:41,890
The magnitude of that weight,

271
00:13:41,890 --> 00:13:47,680
is how strongly that little person feels about the prediction, right?

272
00:13:47,680 --> 00:13:51,250
So, you know, contains add as three because maybe like

273
00:13:51,250 --> 00:13:53,020
"@" signs generally do occur in

274
00:13:53,020 --> 00:13:55,810
email addresses but you know the fraction of alphanumeric characters,

275
00:13:55,810 --> 00:13:57,190
it's you know, less.

276
00:13:57,190 --> 00:13:58,510
So at that level,

277
00:13:58,510 --> 00:14:03,160
you can have some intuition but the precise numbers and y is 0.6 versus 0.5.

278
00:14:03,160 --> 00:14:07,525
Um, that's, um, you can't really say much about that. Yeah. Another question?

279
00:14:07,525 --> 00:14:11,635
Does, uh, [inaudible] [NOISE] is it the same dot product for deeper networks.

280
00:14:11,635 --> 00:14:14,590
They can feel like more weight vectors afterwards.

281
00:14:14,590 --> 00:14:19,330
It's still like it's, like just more than one products. [NOISE]

282
00:14:19,330 --> 00:14:22,390
So right now we're focusing on linear classifier.

283
00:14:22,390 --> 00:14:25,900
So the question is what happens if you have a neural net with more layers?

284
00:14:25,900 --> 00:14:28,150
Um, there's gonna be more dot products but there's

285
00:14:28,150 --> 00:14:30,625
also goin- it's not just adding more features.

286
00:14:30,625 --> 00:14:35,270
There's gonna be other uh, components which we'll get to in a later lecture.

287
00:14:35,270 --> 00:14:37,000
Yeah?

288
00:14:37,000 --> 00:14:38,770
Do the weights have to add up to a certain number or

289
00:14:38,770 --> 00:14:43,000
how do you normalize it, so the weights, like you have to change the score value

290
00:14:43,000 --> 00:14:43,795
[inaudible] .

291
00:14:43,795 --> 00:14:45,535
Yeah. So the question is,

292
00:14:45,535 --> 00:14:48,685
do the weights have to add up to something? Short answer is.

293
00:14:48,685 --> 00:14:52,120
No. There's obviously restricted settings,

294
00:14:52,120 --> 00:14:55,750
where you might want to normalize the weights or something but we're not gonna,

295
00:14:55,750 --> 00:14:58,610
you know, uh, consider that right now.

296
00:14:58,610 --> 00:15:03,400
Later, we'll see that the magnitude of weight does tell you, you know, something.

297
00:15:03,830 --> 00:15:10,290
Okay, so, so just to summarize it's important to note that the weight vectors,

298
00:15:10,290 --> 00:15:12,730
there's only one weight vector, right, you have to find

299
00:15:12,730 --> 00:15:16,195
one set of parameters for every- everybody.

300
00:15:16,195 --> 00:15:18,655
But the feature vector is per example.

301
00:15:18,655 --> 00:15:21,700
So for every input, you get a new feature vector and

302
00:15:21,700 --> 00:15:27,500
the dot product of those two weighted combination of features is the uh, is the score.

303
00:15:27,540 --> 00:15:34,945
Okay, so, so now let's try to put the pieces together and define,

304
00:15:34,945 --> 00:15:38,545
um, uh, of the actual predictor.

305
00:15:38,545 --> 00:15:41,380
All right, so remember we had this box with f in it,

306
00:15:41,380 --> 00:15:43,405
which takes x and returns y.

307
00:15:43,405 --> 00:15:45,925
So what is inside that box?

308
00:15:45,925 --> 00:15:48,370
Um, and I'm hopefully giving you some intuition.

309
00:15:48,370 --> 00:15:50,350
Let me go to a board and write, uh, a few more things.

310
00:15:50,350 --> 00:15:51,820
So the score, uh,

311
00:15:51,820 --> 00:15:54,190
remember is w dot phi of x.

312
00:15:54,190 --> 00:15:59,785
And this is just gonna be a number, um, and uh, the predictor.

313
00:15:59,785 --> 00:16:05,305
So linear predictor actually let me call this linear.

314
00:16:05,305 --> 00:16:09,385
To be more precise, it's a linear classifier not just a predictor.

315
00:16:09,385 --> 00:16:13,060
Classifier is just a predictor that does classification.

316
00:16:13,060 --> 00:16:17,810
Um, so a linear classifier

317
00:16:18,210 --> 00:16:21,190
um, denoted f of w.

318
00:16:21,190 --> 00:16:23,650
So f is where we're going to use, you know, predictors.

319
00:16:23,650 --> 00:16:27,970
W just means that this predictor depends on a particular set of weights.

320
00:16:27,970 --> 00:16:30,895
And this predictor is, uh,

321
00:16:30,895 --> 00:16:34,660
going to look at the score and return the sign of that score.

322
00:16:34,660 --> 00:16:35,680
So what is the sign?

323
00:16:35,680 --> 00:16:39,100
The sign looks at the score and says, is it a positive of a number?

324
00:16:39,100 --> 00:16:41,830
if it's positive then we're gonna return plus 1.

325
00:16:41,830 --> 00:16:44,440
If it's a negative number, I'm gonna return minus 1.

326
00:16:44,440 --> 00:16:46,720
And if it's 0 then you know, I don't care.

327
00:16:46,720 --> 00:16:49,345
You can return plus 1 if you want, it doesn't matter.

328
00:16:49,345 --> 00:16:55,330
Um, so what this is doing the remember the score is either, is a real number.

329
00:16:55,330 --> 00:16:59,260
So it's either gonna be kind of leaning towards um, you know,

330
00:16:59,260 --> 00:17:02,560
large value, large positive values or leaning towards,

331
00:17:02,560 --> 00:17:05,785
uh, s- large small- negative values.

332
00:17:05,785 --> 00:17:10,630
And the sign basically says, okay you gotta commit are you- which side are you on?

333
00:17:10,630 --> 00:17:12,670
Are you on the positive side or you on the negative side?

334
00:17:12,670 --> 00:17:13,990
And just kind of discretizes it.

335
00:17:13,990 --> 00:17:16,820
That's what the sign does.

336
00:17:17,520 --> 00:17:19,060
Okay.

337
00:17:19,060 --> 00:17:25,120
Okay, so, so let's look at a simple example

338
00:17:25,120 --> 00:17:29,350
because I think a lot of what I've seen before is kind of more the,

339
00:17:29,350 --> 00:17:34,060
uh, formal machinery behind and the math behind how it works but it's

340
00:17:34,060 --> 00:17:39,190
really useful to have some geometric intuition because then you can draw some pictures.

341
00:17:39,190 --> 00:17:41,620
Okay, so let's consider this, uh, case.

342
00:17:41,620 --> 00:17:44,410
So we have a weight vector which is 2, 1,

343
00:17:44,410 --> 00:17:48,490
2 minus 1, and a feature vector which is a 2, 0,

344
00:17:48,490 --> 00:17:51,565
and another feature vector which is x0, 2 and 2, 4.

345
00:17:51,565 --> 00:17:55,705
Okay. So there's only two dimensions so I can try to draw them on a board.

346
00:17:55,705 --> 00:17:57,490
So let's try to do that.

347
00:17:57,490 --> 00:18:01,945
Okay, so here is a two-dimensional plot.

348
00:18:01,945 --> 00:18:05,725
Um, and let's draw the fea- the weight vector first.

349
00:18:05,725 --> 00:18:09,760
Okay so the weight vector is going to be at 2 minus 1.

350
00:18:09,760 --> 00:18:12,445
Okay. So that's this point.

351
00:18:12,445 --> 00:18:15,925
And the way to think about the weight vector is not the point.

352
00:18:15,925 --> 00:18:18,460
Um, but actually um,

353
00:18:18,460 --> 00:18:22,030
the, the, the vector going from the origin to that point

354
00:18:22,030 --> 00:18:24,205
for reasons that will become clear later.

355
00:18:24,205 --> 00:18:26,365
Okay so that's the, that's the weight.

356
00:18:26,365 --> 00:18:31,600
Okay. Um and then what about the other points so we have 2, 0, 0, 2.

357
00:18:31,600 --> 00:18:35,035
So 2, 0 is here,

358
00:18:35,035 --> 00:18:43,135
0, 2 is here and 2, 4 is, uh, here.

359
00:18:43,135 --> 00:18:45,340
Right? Okay, so we have three points here.

360
00:18:45,340 --> 00:18:52,780
Okay, so, um, how do I think about what this weight vectors is, is doing?

361
00:18:52,780 --> 00:18:57,220
So just for just for reference remember the classifier is looking

362
00:18:57,220 --> 00:19:02,120
at the sign of W dot, uh, phi of x.

363
00:19:02,280 --> 00:19:08,410
Okay. Um, so let's try to do uh, classification on these three points.

364
00:19:08,410 --> 00:19:10,000
Okay so w is um,

365
00:19:10,000 --> 00:19:13,960
let me write it out formally, so 2, 1.

366
00:19:13,960 --> 00:19:16,105
Um, and this is 0, 2.

367
00:19:16,105 --> 00:19:25,255
So what's the score when I do W dot phi of x here? It's 4, right?

368
00:19:25,255 --> 00:19:29,350
Because this is um, uh,

369
00:19:29,350 --> 00:19:35,980
2, 0, 0, 2 um, 2, 4.

370
00:19:35,980 --> 00:19:38,080
So this is just a dot product that's 4,

371
00:19:38,080 --> 00:19:41,500
um, and take the sign what's the sign of 4?

372
00:19:41,500 --> 00:19:42,220
One.

373
00:19:42,220 --> 00:19:49,705
Okay. So that means I'm going to label this point as a positive, right?

374
00:19:49,705 --> 00:19:52,885
Positive point, okay what about 0, 2?

375
00:19:52,885 --> 00:19:55,690
Actually, sorry, this is just be a minus 1, right?

376
00:19:55,690 --> 00:19:58,195
Okay. This is 2, minus 1.

377
00:19:58,195 --> 00:20:00,715
Okay, so if I take the dot product between this,

378
00:20:00,715 --> 00:20:06,380
I get minus 2 and then the sign of minus 2 is,

379
00:20:06,660 --> 00:20:11,305
is minus 1, okay, so that's a minus.

380
00:20:11,305 --> 00:20:12,580
Um, and what about this one?

381
00:20:12,580 --> 00:20:16,165
So what's the dot product there? It's gonna be 0.

382
00:20:16,165 --> 00:20:24,010
Okay. So, um, so this classifier will classify this point as a positive.

383
00:20:24,010 --> 00:20:26,215
This is a negative and this one I don't know.

384
00:20:26,215 --> 00:20:28,855
Okay. So we can fill in more points.

385
00:20:28,855 --> 00:20:31,510
Um, but, but, you know,

386
00:20:31,510 --> 00:20:35,005
does anyone see kind of um, maybe a more general pattern?

387
00:20:35,005 --> 00:20:37,990
I don't wanna have to fill in the entire board with classifications.

388
00:20:37,990 --> 00:20:39,865
Yeah?

389
00:20:39,865 --> 00:20:45,295
Orthogonal, everything to the right of it is positive and to the left of it is negative.

390
00:20:45,295 --> 00:20:49,540
Yeah so so let's try to draw the orthogonal.

391
00:20:49,540 --> 00:20:51,985
Uh, this needs to go through that line.

392
00:20:51,985 --> 00:21:00,520
Okay, [NOISE] okay, so let's draw the orthogonal.

393
00:21:00,520 --> 00:21:03,850
So this is a right angle. Okay.

394
00:21:03,850 --> 00:21:07,990
And, ah, what that gentleman said is that,

395
00:21:07,990 --> 00:21:14,410
the points- any point over here because it has acute angle width w,

396
00:21:14,410 --> 00:21:17,005
is going to be classified as positive.

397
00:21:17,005 --> 00:21:19,240
So all of this stuff is um, you know,

398
00:21:19,240 --> 00:21:22,270
positive, positive, positive, positive, positive,

399
00:21:22,270 --> 00:21:28,450
and everything over here because it's an obtuse angle with w is going to be negative,

400
00:21:28,450 --> 00:21:30,715
so everything over here is negative.

401
00:21:30,715 --> 00:21:37,165
And then, everything on this line is going to be 0.

402
00:21:37,165 --> 00:21:41,405
Okay? So, so I don't know.

403
00:21:41,405 --> 00:21:44,490
Okay, and this line is called, um,

404
00:21:44,490 --> 00:21:50,190
the decision boundary, which is the concept not just for linear classifiers,

405
00:21:50,190 --> 00:21:52,544
but whenever you have any sort of classifier

406
00:21:52,544 --> 00:21:55,035
the decision boundary is the separation between

407
00:21:55,035 --> 00:22:00,625
the regions of the space where the classification is positive versus negative.

408
00:22:00,625 --> 00:22:04,030
Okay? And in this case, um,

409
00:22:04,030 --> 00:22:09,685
it's, it's separate because uh, we have linear classifiers,

410
00:22:09,685 --> 00:22:11,560
the decision boundary is straight,

411
00:22:11,560 --> 00:22:15,865
and we're just separating the, the space into two halves.

412
00:22:15,865 --> 00:22:18,940
Um, if you were in three-dimensions, um,

413
00:22:18,940 --> 00:22:21,835
this vector would still be just a you know vector,

414
00:22:21,835 --> 00:22:24,340
but this decision, um,

415
00:22:24,340 --> 00:22:26,755
boundary would be a plane.

416
00:22:26,755 --> 00:22:29,680
So you can think about it as you know coming out of the board if you want,

417
00:22:29,680 --> 00:22:31,945
but I'm not gonna try to draw that.

418
00:22:31,945 --> 00:22:38,050
Um, and that's, that's kind of the geometric interpretation of how linear classifiers,

419
00:22:38,050 --> 00:22:40,330
ah, you know, work here. Question, yeah?

420
00:22:40,330 --> 00:22:43,420
It seems like your weight could be any values here.

421
00:22:43,420 --> 00:22:44,170
Right?

422
00:22:44,170 --> 00:22:44,591
Yeah.

423
00:22:44,591 --> 00:22:46,000
So we have one last [inaudible].

424
00:22:46,000 --> 00:22:47,000
Yeah.

425
00:22:47,000 --> 00:22:49,940
[inaudible] .

426
00:22:49,940 --> 00:22:53,755
Yeah. So that's a good point. So the, the observation is that,

427
00:22:53,755 --> 00:22:57,730
no matter, if you scale this weight by 2,

428
00:22:57,730 --> 00:23:00,490
it's actually gonna still have the same decision boundaries.

429
00:23:00,490 --> 00:23:04,450
So the magnitude of the weight doesn't matter it's the direction that matters.

430
00:23:04,450 --> 00:23:08,815
Um, so this is true for just making a prediction.

431
00:23:08,815 --> 00:23:10,960
Um, when we look at learning, ah,

432
00:23:10,960 --> 00:23:14,380
the magnitude of the weight will matter because we're going to,

433
00:23:14,380 --> 00:23:18,770
you know, consider other more nuanced loss functions.

434
00:23:19,170 --> 00:23:25,150
Yeah. Okay. So let's move on.

435
00:23:25,150 --> 00:23:26,620
Any questions about linear predictors?

436
00:23:26,620 --> 00:23:28,525
So, so, far what we've done is,

437
00:23:28,525 --> 00:23:30,235
we haven't done any learning.

438
00:23:30,235 --> 00:23:33,100
Right. If you've ah, you know, noticed,

439
00:23:33,100 --> 00:23:37,585
we've just simply defined the set of predictors that we're interested in.

440
00:23:37,585 --> 00:23:39,100
So we have a feature vector,

441
00:23:39,100 --> 00:23:43,450
we have weight vectors, multiply them together, get a score and

442
00:23:43,450 --> 00:23:48,430
then you can send them through a sign function and you get these linear classifiers.

443
00:23:48,430 --> 00:23:52,420
Right. There, there's no specification of data yet.

444
00:23:52,420 --> 00:23:57,970
Okay. So now, let's actually turn to do some learning.

445
00:23:57,970 --> 00:24:00,055
So remember this framework,

446
00:24:00,055 --> 00:24:06,775
learning needs to take some data and return a predictor and our predictors are ah,

447
00:24:06,775 --> 00:24:08,740
specified by a weight vector.

448
00:24:08,740 --> 00:24:11,650
So you can equivalently think about the learning algorithm as

449
00:24:11,650 --> 00:24:16,750
outputting a weight vector if you want for linear classifiers.

450
00:24:16,750 --> 00:24:19,465
Um, and let's unpack the learner.

451
00:24:19,465 --> 00:24:25,390
So the learning algorithm is going to be based on optimization which we started ah,

452
00:24:25,390 --> 00:24:28,300
reviewing last lecture um,

453
00:24:28,300 --> 00:24:33,400
which separates ah, what you want to compute from how you want to compute it.

454
00:24:33,400 --> 00:24:38,530
So we're going to first define an optimization problem which specifies

455
00:24:38,530 --> 00:24:43,930
what properties we want a- a classifier to have in terms of the data,

456
00:24:43,930 --> 00:24:47,350
and then we're going to figure out how to actually optimize this.

457
00:24:47,350 --> 00:24:51,895
[NOISE] And this module is actually really really powerful um,

458
00:24:51,895 --> 00:24:57,835
and it allows people to go ahead and work on different types of

459
00:24:57,835 --> 00:25:00,955
criteria for and different types of models

460
00:25:00,955 --> 00:25:04,960
separately from the people who actually develop general purpose algorithms.

461
00:25:04,960 --> 00:25:08,680
Um, and this has served kind of the field of machinery quite well.

462
00:25:08,680 --> 00:25:13,015
Okay. So let's start with an optimization problem.

463
00:25:13,015 --> 00:25:16,045
So this is an important concept um,

464
00:25:16,045 --> 00:25:20,830
called a loss function and

465
00:25:20,830 --> 00:25:25,915
this is a super general idea that's using the machine learning and statistics.

466
00:25:25,915 --> 00:25:33,265
So a loss function takes a particular example x, y and a weight vector, um,

467
00:25:33,265 --> 00:25:36,460
and returns a number and this number represents

468
00:25:36,460 --> 00:25:39,940
how unhappy we would be if we used the predictor

469
00:25:39,940 --> 00:25:47,255
given by W to make a prediction on x when the correct output is y.

470
00:25:47,255 --> 00:25:49,890
Okay. So it's a little bit of a mouthful but, um,

471
00:25:49,890 --> 00:25:52,785
this basically is trying to characterize, you know,

472
00:25:52,785 --> 00:25:54,525
if you handed me a classifier,

473
00:25:54,525 --> 00:25:57,705
and I go on to this example and try to classify it,

474
00:25:57,705 --> 00:26:00,200
is it gonna get it right or is it gonna get it wrong?

475
00:26:00,200 --> 00:26:01,870
So high loss is bad ah,

476
00:26:01,870 --> 00:26:03,670
you don't wanna lose and

477
00:26:03,670 --> 00:26:05,005
low loss is good.

478
00:26:05,005 --> 00:26:09,920
So normally, zero loss is the- the best you can then hope for.

479
00:26:10,050 --> 00:26:16,765
Okay. So let's do figure out the loss function for binary classification here.

480
00:26:16,765 --> 00:26:19,270
Um, so just some notation,

481
00:26:19,270 --> 00:26:20,635
the correct label is, ah,

482
00:26:20,635 --> 00:26:23,710
denoted y and, um,

483
00:26:23,710 --> 00:26:27,850
the predicted label remember is um, the score, ah,

484
00:26:27,850 --> 00:26:32,620
sent through the sign function and that's going to give you some particular label.

485
00:26:32,620 --> 00:26:35,290
Um, and let's look at this example.

486
00:26:35,290 --> 00:26:39,265
So w equals 2 minus 1 phi of x equals ah,

487
00:26:39,265 --> 00:26:41,530
2, 0 and y equals minus 1.

488
00:26:41,530 --> 00:26:46,210
Okay. So we already defined the score as, um,

489
00:26:46,210 --> 00:26:49,675
one example is a w dot phi of x which is,

490
00:26:49,675 --> 00:26:53,950
um, how co- confident we're predicting minu- plus 1.

491
00:26:53,950 --> 00:26:55,390
That's the way to, uh,

492
00:26:55,390 --> 00:26:56,890
you know, interpret this.

493
00:26:56,890 --> 00:27:00,040
Okay. So um, what's the score of this,

494
00:27:00,040 --> 00:27:02,500
for this particular example again?

495
00:27:02,500 --> 00:27:03,400
It's 4.

496
00:27:03,400 --> 00:27:06,985
Right. Um, which means I'm kind of,

497
00:27:06,985 --> 00:27:08,845
kinda positive that it's ah,

498
00:27:08,845 --> 00:27:10,705
you know, a plus 1. Yeah. Question?

499
00:27:10,705 --> 00:27:15,100
Ah, I was wondering, is the loss function generally 1-dimensional or,

500
00:27:15,100 --> 00:27:17,215
or the output of the loss function?

501
00:27:17,215 --> 00:27:19,900
Yeah. So the- the question is whether the output of

502
00:27:19,900 --> 00:27:23,110
loss function is usually a single number or not.

503
00:27:23,110 --> 00:27:25,795
Um, in most cases it is for

504
00:27:25,795 --> 00:27:27,730
basically all practical cases you should

505
00:27:27,730 --> 00:27:30,310
think about the loss functions outputting a single number.

506
00:27:30,310 --> 00:27:31,900
The inputs can be, you know,

507
00:27:31,900 --> 00:27:36,460
a crazy high-dimensional. Yeah.

508
00:27:36,460 --> 00:27:37,960
Why is it not 1-dimension?

509
00:27:37,960 --> 00:27:41,170
[NOISE] Um, there are cases where you might have

510
00:27:41,170 --> 00:27:44,785
multiple objectives that you're trying to optimize at once ah,

511
00:27:44,785 --> 00:27:47,935
but in this class it's always gonna be, you know, 1-dimensional.

512
00:27:47,935 --> 00:27:49,915
Like maybe you care about, you know,

513
00:27:49,915 --> 00:27:54,310
both time and space or accuracy but robustness or something.

514
00:27:54,310 --> 00:27:57,055
Sometimes you have multi-objective optimization.

515
00:27:57,055 --> 00:28:00,200
But that's way beyond the scope of this class.

516
00:28:01,200 --> 00:28:04,300
Okay. So we have a score.

517
00:28:04,300 --> 00:28:07,120
Um, and now we're gonna define a margin.

518
00:28:07,120 --> 00:28:09,790
So let me, um.

519
00:28:09,790 --> 00:28:13,550
Okay. So let's, let's actually do this.

520
00:28:13,560 --> 00:28:16,825
So we're talking about classification.

521
00:28:16,825 --> 00:28:19,330
I'm gonna sneak regression in a bit.

522
00:28:19,330 --> 00:28:22,975
So score is w dot phi of x.

523
00:28:22,975 --> 00:28:26,500
This is how confident we are about plus 1, um,

524
00:28:26,500 --> 00:28:32,650
and the margin is the score ah, times y.

525
00:28:32,650 --> 00:28:36,685
Um, and this relies on y being plus 1 or minus 1.

526
00:28:36,685 --> 00:28:39,985
So this might seem a little bit mysterious but let's try to,

527
00:28:39,985 --> 00:28:43,090
you know, decipher that, um here.

528
00:28:43,090 --> 00:28:46,555
Um, so in this example,

529
00:28:46,555 --> 00:28:48,250
the score is 4.

530
00:28:48,250 --> 00:28:50,630
So what's the margin?

531
00:28:52,530 --> 00:28:55,210
You multiply by minus 1.

532
00:28:55,210 --> 00:28:57,835
So the margin is, ah, minus 4.

533
00:28:57,835 --> 00:29:03,130
Right. And the margins interpretation is how correct we are.

534
00:29:03,130 --> 00:29:07,075
Right. So imagine the correct answer is ah,

535
00:29:07,075 --> 00:29:09,910
if, if the score in the margin had the same sign,

536
00:29:09,910 --> 00:29:12,985
then you're gonna get positive numbers and then the,

537
00:29:12,985 --> 00:29:16,225
the confident, the more confident you are then the more correct you are.

538
00:29:16,225 --> 00:29:21,895
Um, but if y is minus 1 and the score is positive,

539
00:29:21,895 --> 00:29:25,885
then the margin is gonna be negative which means that

540
00:29:25,885 --> 00:29:30,400
you're gonna be confidently wrong um, which is bad.

541
00:29:30,400 --> 00:29:32,950
[LAUGHTER]

542
00:29:32,950 --> 00:29:36,475
Okay. So just to to see if we kind of understand what's going on.

543
00:29:36,475 --> 00:29:41,920
Um, so when is a binary classifier making a mistake on a given example.

544
00:29:41,920 --> 00:29:45,760
Um, so I'm gonna ask for a kind of a show of hands.

545
00:29:45,760 --> 00:29:51,340
How many people think it's, it's when the margin is, uh, less than 0.

546
00:29:51,340 --> 00:29:54,370
Okay. I guess we can kind of stop there.

547
00:29:54,370 --> 00:29:57,310
[LAUGHTER]

548
00:29:57,310 --> 00:29:59,620
I used to do these online quizzes where it

549
00:29:59,620 --> 00:30:02,320
was anonymous but we're not doing that this year.

550
00:30:02,320 --> 00:30:05,305
Okay. So yes, the margin is less than 0.

551
00:30:05,305 --> 00:30:08,620
Um, when the margin is less than 0 that means y and

552
00:30:08,620 --> 00:30:12,310
the score are different signs which means that you're making a mistake.

553
00:30:12,310 --> 00:30:15,490
[NOISE]

554
00:30:15,490 --> 00:30:17,920
Okay. So now we have the notion of a margin.

555
00:30:17,920 --> 00:30:20,140
Let's define ah, something called the

556
00:30:20,140 --> 00:30:24,370
zero-one loss and it's called zero-one because it returns either a 0 or a 1.

557
00:30:24,370 --> 00:30:26,110
Okay. Very creative name.

558
00:30:26,110 --> 00:30:34,210
Um, so the loss function is simply,

559
00:30:34,210 --> 00:30:36,100
did you make a mistake or not?

560
00:30:36,100 --> 00:30:38,890
Okay. So this notation let's try to decipher a bit.

561
00:30:38,890 --> 00:30:44,410
So if f of x here is the prediction when the input is x,

562
00:30:44,410 --> 00:30:48,330
um, and not equal y is saying, did you make a mistake?

563
00:30:48,330 --> 00:30:50,040
So that's, think about it as a Boolean,

564
00:30:50,040 --> 00:30:53,280
and this one bracket is um, just notation.

565
00:30:53,280 --> 00:30:58,665
It's called an indicator function that takes a condition and returns either a 1 or 0.

566
00:30:58,665 --> 00:31:01,445
So if ah, if the,

567
00:31:01,445 --> 00:31:03,370
the condition is true,

568
00:31:03,370 --> 00:31:07,015
then it's gonna return a 1 and if the condition is false, it returns a 0.

569
00:31:07,015 --> 00:31:10,930
Okay. So all this is doing is basically returning a 1,

570
00:31:10,930 --> 00:31:12,670
if you made a mistake and 0,

571
00:31:12,670 --> 00:31:14,845
if you didn't make a mistake.

572
00:31:14,845 --> 00:31:19,420
Okay. And we can write that as follows.

573
00:31:19,420 --> 00:31:22,030
We can write that as um,

574
00:31:22,030 --> 00:31:24,940
the margin less or equal to 0.

575
00:31:24,940 --> 00:31:28,510
Right. Because pre- on the previous side of the margin is less than or equal to 0,

576
00:31:28,510 --> 00:31:32,025
then we've made a mistake and we should incur ah,

577
00:31:32,025 --> 00:31:36,480
a loss of 1 and if the margin is greater than 0,

578
00:31:36,480 --> 00:31:40,300
then we didn't make a mistake and we should incur a loss of 0.

579
00:31:42,750 --> 00:31:47,440
Okay. All right so, um,

580
00:31:47,440 --> 00:31:50,830
it will be useful to draw these loss functions,

581
00:31:50,830 --> 00:31:52,645
um, pictorially like this.

582
00:31:52,645 --> 00:31:55,870
Okay, so on the axi- x-axis here,

583
00:31:55,870 --> 00:31:58,570
we're going to show the margin, right?

584
00:31:58,570 --> 00:32:02,080
Remember the margin is how, uh, correct you are.

585
00:32:02,080 --> 00:32:03,655
And on the, uh,

586
00:32:03,655 --> 00:32:05,620
y-axis we're gonna show the-

587
00:32:05,620 --> 00:32:09,415
the loss function which is how much you're gonna suffer for it.

588
00:32:09,415 --> 00:32:11,260
Okay, so remember the margin,

589
00:32:11,260 --> 00:32:12,730
if the margin is positive,

590
00:32:12,730 --> 00:32:16,360
that means you're getting it right which means that the loss is 0.

591
00:32:16,360 --> 00:32:19,360
But if the margin is less than 0,

592
00:32:19,360 --> 00:32:22,315
that means you are getting it wrong and the loss is 1.

593
00:32:22,315 --> 00:32:24,355
Okay, so this is a 0-1 loss.

594
00:32:24,355 --> 00:32:26,830
That's, uh, thi- this thing- the visual that

595
00:32:26,830 --> 00:32:29,410
you should have in mind when you think about zero-one loss. Yeah.

596
00:32:29,410 --> 00:32:32,545
[NOISE] Like less than 0

597
00:32:32,545 --> 00:32:37,690
because we are not defining the event actually 0 [inaudible] classified as correct.

598
00:32:37,690 --> 00:32:42,520
Yeah, so there is this kind of boundary condition of when ex- what happens exactly

599
00:32:42,520 --> 00:32:47,335
at 0 that I'm trying to sweep under the rug because it's not, um, terribly important.

600
00:32:47,335 --> 00:32:50,770
Um, here, it's less we go to 0 to be kind of on the safe side.

601
00:32:50,770 --> 00:32:52,180
So if you don't know you're also,

602
00:32:52,180 --> 00:32:54,580
uh, gonna get it wrong.

603
00:32:54,580 --> 00:32:58,270
Um, otherwise you could always

604
00:32:58,270 --> 00:33:02,960
just return 0 and then you, that, you don't want that.

605
00:33:05,400 --> 00:33:08,140
Okay. So is it- uh,

606
00:33:08,140 --> 00:33:09,430
any questions about, uh,

607
00:33:09,430 --> 00:33:11,260
kind of binary classification so far.

608
00:33:11,260 --> 00:33:13,930
So we've set up these linear predictors and I've defined

609
00:33:13,930 --> 00:33:17,815
the 0-1 loss as a way to capture, um,

610
00:33:17,815 --> 00:33:22,300
how unhappy we would be if we had a classifier that was,

611
00:33:22,300 --> 00:33:26,180
ah, operating on a particular data point x, y.

612
00:33:27,720 --> 00:33:32,335
So, um, just to-

613
00:33:32,335 --> 00:33:38,660
I'm gonna go on a little bit of a digression and talk about linear regression.

614
00:33:38,910 --> 00:33:45,820
Um, uh, um, [NOISE]

615
00:33:45,820 --> 00:33:48,430
and, and the reason I'm doing this is that

616
00:33:48,430 --> 00:33:51,940
loss minimization is such a powerful and general framework,

617
00:33:51,940 --> 00:33:53,770
and it go- transcends, you know,

618
00:33:53,770 --> 00:33:55,120
all of these, you know,

619
00:33:55,120 --> 00:33:57,085
linear classifiers, regression, setups.

620
00:33:57,085 --> 00:34:01,060
So I want to kind of emphasize over- the overall story.

621
00:34:01,060 --> 00:34:05,230
So I'm gonna give you a bunch of different examples, um, classification,

622
00:34:05,230 --> 00:34:09,565
linear regression side-by-side so we can actually see how they compare and hopefully,

623
00:34:09,565 --> 00:34:14,320
their- the common denominator will kind of emerge more, um, clearly from that.

624
00:34:14,320 --> 00:34:18,580
Okay, so we talked a little bit about linear regression in the last lecture, right?

625
00:34:18,580 --> 00:34:20,920
So linear regression in some sense is simpler than

626
00:34:20,920 --> 00:34:24,625
classification because if you have a linear,

627
00:34:24,625 --> 00:34:27,460
uh, uh, predictor, um,

628
00:34:27,460 --> 00:34:29,620
and you get the score w dot phi of x,

629
00:34:29,620 --> 00:34:31,375
it's already a real number.

630
00:34:31,375 --> 00:34:32,530
So in linear regression,

631
00:34:32,530 --> 00:34:36,820
you simply return that real number and you call that your prediction.

632
00:34:36,820 --> 00:34:43,090
Okay? Okay so now we- let's move towards defining our loss function.

633
00:34:43,090 --> 00:34:44,995
Um, so there's gonna be, uh,

634
00:34:44,995 --> 00:34:47,154
a concept that's gonna be useful,

635
00:34:47,154 --> 00:34:50,680
it's called the residual, um, which is,

636
00:34:50,680 --> 00:34:53,095
as- against kind of trying to capture how,

637
00:34:53,095 --> 00:34:54,190
uh, wrong you are.

638
00:34:54,190 --> 00:35:00,670
Um, so here is a particular linear, uh, predictor, um,

639
00:35:00,670 --> 00:35:05,950
linear regresser, um, and it's making predictions all along,

640
00:35:05,950 --> 00:35:07,390
you know, for different values of x.

641
00:35:07,390 --> 00:35:10,480
Um, and here's a data point of Phi of xy.

642
00:35:10,480 --> 00:35:14,005
Okay? So the residual is the difference between,

643
00:35:14,005 --> 00:35:17,950
um, the true value y and the predictor value y.

644
00:35:17,950 --> 00:35:23,065
Okay, um, and in particular it's the amount by which,

645
00:35:23,065 --> 00:35:28,160
um, the prediction is overshooting the, you know, target.

646
00:35:28,170 --> 00:35:31,315
Okay, so this is- this is a difference.

647
00:35:31,315 --> 00:35:37,540
Um, and if you square the [NOISE] difference you get something called,

648
00:35:37,540 --> 00:35:39,010
uh, the squared loss.

649
00:35:39,010 --> 00:35:42,160
[NOISE]

650
00:35:42,160 --> 00:35:44,830
So this is something we mentioned last lecture.

651
00:35:44,830 --> 00:35:48,550
Um, residual can be either negative or [NOISE] positive.

652
00:35:48,550 --> 00:35:50,845
Um, but errors, either,

653
00:35:50,845 --> 00:35:53,020
if you're very positive or very negative,

654
00:35:53,020 --> 00:35:56,260
that's bad and squaring them makes it so that you're gonna, you know,

655
00:35:56,260 --> 00:35:58,435
suffer equally for, um,

656
00:35:58,435 --> 00:36:01,340
errors in both, you know, directions.

657
00:36:01,950 --> 00:36:05,650
Okay, so the square loss is the residual squared.

658
00:36:05,650 --> 00:36:08,140
So let's do this kind of simple example.

659
00:36:08,140 --> 00:36:11,365
So here we have our weight vector 2 minus 1.

660
00:36:11,365 --> 00:36:14,530
The feature vector is 2, 0. What's the score?

661
00:36:14,530 --> 00:36:18,910
It's 4, y is minus 1.

662
00:36:18,910 --> 00:36:25,120
So, uh, the residual is 4 minus minus 1 which is 5 and,

663
00:36:25,120 --> 00:36:26,935
uh, 5 squared is 25.

664
00:36:26,935 --> 00:36:31,190
So the squared loss on this particular example is 25.

665
00:36:33,000 --> 00:36:37,765
Okay, so let's plot this.

666
00:36:37,765 --> 00:36:39,790
So just like we did it for a 0-1 loss.

667
00:36:39,790 --> 00:36:41,860
Let's see what this loss function looks like.

668
00:36:41,860 --> 00:36:44,500
So the, the horizontal axis here

669
00:36:44,500 --> 00:36:47,480
instead of being the margin is going to be this quantity,

670
00:36:47,480 --> 00:36:50,055
uh, for regression called the residual.

671
00:36:50,055 --> 00:36:54,075
Um, it's going to be the difference between the prediction and the, the true target.

672
00:36:54,075 --> 00:36:57,150
And I'm gonna plot the loss function.

673
00:36:57,150 --> 00:36:59,700
Um, and this loss function is just,

674
00:36:59,700 --> 00:37:01,940
you know, the squared function, right?

675
00:37:01,940 --> 00:37:03,970
So with- if the residual is 0,

676
00:37:03,970 --> 00:37:05,485
then the loss is 0.

677
00:37:05,485 --> 00:37:08,845
If as a residual grows in either direction,

678
00:37:08,845 --> 00:37:10,465
then I'm going to pay,

679
00:37:10,465 --> 00:37:12,505
uh, something for it.

680
00:37:12,505 --> 00:37:15,820
And it's a quadratic penalty which means that,

681
00:37:15,820 --> 00:37:17,350
um, it actually grows,

682
00:37:17,350 --> 00:37:19,270
you know, uh, pretty fast.

683
00:37:19,270 --> 00:37:20,335
So if I'm, you know,

684
00:37:20,335 --> 00:37:23,990
the residual is 10 then I'm paying 100.

685
00:37:24,930 --> 00:37:28,765
Okay, so, so that's the squared loss.

686
00:37:28,765 --> 00:37:30,910
Um, there's also another loss.

687
00:37:30,910 --> 00:37:32,410
I'll throw in here,

688
00:37:32,410 --> 00:37:34,825
um, called the absolute deviation loss.

689
00:37:34,825 --> 00:37:39,865
And this might actually be the last thought, if you didn't know about regression you might,

690
00:37:39,865 --> 00:37:41,440
uh, immediately come to.

691
00:37:41,440 --> 00:37:45,790
It's basically the absolute difference between the prediction and,

692
00:37:45,790 --> 00:37:49,090
um, the, the actual true target.

693
00:37:49,090 --> 00:37:52,075
[NOISE] Um, turns out the squared loss.

694
00:37:52,075 --> 00:37:54,745
The- there's a kind of a longer discussion about,

695
00:37:54,745 --> 00:37:56,410
you know, which loss function,

696
00:37:56,410 --> 00:37:57,775
um, you know, makes sense.

697
00:37:57,775 --> 00:38:00,355
The- the salient points here are that

698
00:38:00,355 --> 00:38:04,720
the absolute deviation loss is kind it has this kink here.

699
00:38:04,720 --> 00:38:07,450
Um, and so it's not smooth.

700
00:38:07,450 --> 00:38:09,610
Sometimes it makes it harder to optimize,

701
00:38:09,610 --> 00:38:14,545
um, but the squared loss also has this kind of thing that blows up,

702
00:38:14,545 --> 00:38:16,570
which means that it's, uh, uh,

703
00:38:16,570 --> 00:38:20,320
it really doesn't like having outliers or, uh,

704
00:38:20,320 --> 00:38:22,525
really large values because it's gonna,

705
00:38:22,525 --> 00:38:24,820
you- you're gonna pay a lot for it.

706
00:38:24,820 --> 00:38:28,180
Um, but at this level,

707
00:38:28,180 --> 00:38:30,370
just think about this as, you know, different losses.

708
00:38:30,370 --> 00:38:33,085
There's also something called a Huber loss which kind of, uh, um,

709
00:38:33,085 --> 00:38:35,410
combines both of these, is smooth,

710
00:38:35,410 --> 00:38:38,230
and also grows linearly instead of quadratically.

711
00:38:38,230 --> 00:38:47,050
Um, okay, so we have both classification and regression.

712
00:38:47,050 --> 00:38:49,570
We can define margins and residuals.

713
00:38:49,570 --> 00:38:51,460
We get either, uh,

714
00:38:51,460 --> 00:38:53,620
different loss functions out of it.

715
00:38:53,620 --> 00:38:58,525
Right? Um, and now we want to minimize the loss.

716
00:38:58,525 --> 00:39:04,405
Okay? Um, so it turns out that for one example and this is really easy, right?

717
00:39:04,405 --> 00:39:05,950
So if I- if I told you,

718
00:39:05,950 --> 00:39:08,695
okay, how do I minimize the loss here?

719
00:39:08,695 --> 00:39:13,345
Well, okay, it's 0. Done. [NOISE] So that- that's not super interesting.

720
00:39:13,345 --> 00:39:15,250
And this corresponds to the fact that,

721
00:39:15,250 --> 00:39:16,720
you know, if you have a classifier,

722
00:39:16,720 --> 00:39:20,350
you're just trying to fit one point, um, it's really not that hard.

723
00:39:20,350 --> 00:39:22,990
So that's kind of not the point.

724
00:39:22,990 --> 00:39:26,005
[NOISE] The point of machine learning is that you have to fit all of them.

725
00:39:26,005 --> 00:39:28,195
Remember, you only get one weight vector,

726
00:39:28,195 --> 00:39:29,350
you have all of these examples,

727
00:39:29,350 --> 00:39:30,370
you have a million examples.

728
00:39:30,370 --> 00:39:33,460
And you want to find one weight vector that kind of balances,

729
00:39:33,460 --> 00:39:35,590
uh, errors across all of them.

730
00:39:35,590 --> 00:39:40,990
And in general, you might not be able to achieve loss of 0, right?

731
00:39:40,990 --> 00:39:42,340
So tough luck . Life is hard.

732
00:39:42,340 --> 00:39:45,550
Ah, so you have to make trade-offs, you know,

733
00:39:45,550 --> 00:39:51,535
which examples are you going to kind of sacrifice for the good of other examples.

734
00:39:51,535 --> 00:39:54,850
And this is kind of actually a lot of where, you know,

735
00:39:54,850 --> 00:39:57,310
issues around fairness of machine learning actually come in

736
00:39:57,310 --> 00:40:00,790
because in cases where you can't actually make a prediction that's,

737
00:40:00,790 --> 00:40:02,770
you know, equally good for everyone.

738
00:40:02,770 --> 00:40:04,450
You know, how do you actually, you know,

739
00:40:04,450 --> 00:40:06,340
responsibly make these trade-offs.

740
00:40:06,340 --> 00:40:07,780
Um, but, you know,

741
00:40:07,780 --> 00:40:09,925
that's a- that's a broader topic.

742
00:40:09,925 --> 00:40:15,910
Let's just focus on trade-off defined by the simple sum over all the loss examples.

743
00:40:15,910 --> 00:40:20,965
So lets just say we want to minimize the average loss over all the examples.

744
00:40:20,965 --> 00:40:22,945
Okay, so once we have these loss functions,

745
00:40:22,945 --> 00:40:25,465
if you average [NOISE] over the training set,

746
00:40:25,465 --> 00:40:28,135
you get something which we're gonna call a train loss.

747
00:40:28,135 --> 00:40:31,600
Um, and that's a function of W. Right?

748
00:40:31,600 --> 00:40:33,670
So loss is on a particular example.

749
00:40:33,670 --> 00:40:35,650
Train loss is on the entire data set.

750
00:40:35,650 --> 00:40:41,310
[NOISE]

751
00:40:41,310 --> 00:40:46,060
Okay. So any questions about this, uh, so far?

752
00:40:51,530 --> 00:40:53,760
Okay. So there is this, uh,

753
00:40:53,760 --> 00:40:57,810
discussion about which regression loss to use, which I'm gonna skip.

754
00:40:57,810 --> 00:41:01,035
Um, you can feel free to read it in the notes if you're interested.

755
00:41:01,035 --> 00:41:05,145
The punchline is that if you want things that look like the mean square loss,

756
00:41:05,145 --> 00:41:06,810
if you want things that look like the median,

757
00:41:06,810 --> 00:41:08,250
use the absolute deviation loss.

758
00:41:08,250 --> 00:41:11,850
Um, but I'll skip that for now. Yeah?

759
00:41:11,850 --> 00:41:15,495
[inaudible] regression like this.

760
00:41:15,495 --> 00:41:21,030
Uh, when do people start thinking of regressions like in terms of loss minimization?

761
00:41:21,030 --> 00:41:21,940
Yeah.

762
00:41:21,940 --> 00:41:23,205
Uh, so regression has,

763
00:41:23,205 --> 00:41:27,585
Least Squares Regression is from like the early 1800s.

764
00:41:27,585 --> 00:41:30,210
Um, so it's been around for is- you know, kind of,

765
00:41:30,210 --> 00:41:32,430
you can call it the first machine learning that was ever done,

766
00:41:32,430 --> 00:41:35,865
um, if you- if you want, um,

767
00:41:35,865 --> 00:41:39,930
I guess the loss minimization framework is,

768
00:41:39,930 --> 00:41:45,345
um, it's hard to kind of pinpoint a particular point in time, you know,

769
00:41:45,345 --> 00:41:49,080
it's kind of not a terribly, uh,

770
00:41:49,080 --> 00:41:51,600
um, er, er, you know, it's not like,

771
00:41:51,600 --> 00:41:52,935
uh, um, you know,

772
00:41:52,935 --> 00:41:54,150
innovation in some sense.

773
00:41:54,150 --> 00:41:58,620
It's just more of a- at least right now it's kind of a pedagogical tool to organize,

774
00:41:58,620 --> 00:42:03,240
um, all the different methods that exist. Yeah.

775
00:42:03,240 --> 00:42:06,495
Say I'm training on mean and median.

776
00:42:06,495 --> 00:42:10,020
Do you mean that like, uh, in that particular training, training set,

777
00:42:10,020 --> 00:42:13,785
the median would be the [NOISE] highest accuracy and the most confident,

778
00:42:13,785 --> 00:42:15,240
whereas like with, uh,

779
00:42:15,240 --> 00:42:19,000
loss [inaudible] deviation would be the median instead of the mean?

780
00:42:19,000 --> 00:42:23,955
Yeah. So, um, I don't wanna get into these examples but, uh, bri- briefly,

781
00:42:23,955 --> 00:42:30,989
if you have three points that you- you can't exactly f- fit perfectly,

782
00:42:30,989 --> 00:42:34,530
um, you- if you use absolute deviation,

783
00:42:34,530 --> 00:42:36,930
then you're gonna find the median value.

784
00:42:36,930 --> 00:42:39,105
You're gonna basically predict the median value.

785
00:42:39,105 --> 00:42:40,830
And if you use the square loss,

786
00:42:40,830 --> 00:42:42,675
you're gonna predict the mean value.

787
00:42:42,675 --> 00:42:47,400
But, um, I'm happy to talk offline [NOISE] if- if you want.

788
00:42:47,400 --> 00:42:51,065
[NOISE]

789
00:42:51,065 --> 00:42:53,160
Okay. So what we've talked about so far is we have

790
00:42:53,160 --> 00:42:58,220
these wonderful linear predictors which are driven by feature vectors and weight vectors,

791
00:42:58,220 --> 00:43:02,720
and now we can define a bunch of different loss functions that capture,

792
00:43:02,720 --> 00:43:05,675
you know, how we care about,

793
00:43:05,675 --> 00:43:08,090
um, you know, regression and classification.

794
00:43:08,090 --> 00:43:12,825
And now let's try to actually do some real, uh, machine learning.

795
00:43:12,825 --> 00:43:15,600
How, how do you actually optimize these objectives?

796
00:43:15,600 --> 00:43:18,435
So remember the learner is going, uh,

797
00:43:18,435 --> 00:43:23,025
so now we've talked about the optimization problem which is minimizing the training loss.

798
00:43:23,025 --> 00:43:25,290
Um, we'll come back to that next lecture.

799
00:43:25,290 --> 00:43:29,025
Um, and then now we're gonna talk about optimization algorithm.

800
00:43:29,025 --> 00:43:33,360
Okay? So what is a optimization problem?

801
00:43:33,360 --> 00:43:35,460
Now, remember last time we said, okay,

802
00:43:35,460 --> 00:43:37,755
let's just abstract away from the details a little bit.

803
00:43:37,755 --> 00:43:39,555
Let's not worry about if it's,

804
00:43:39,555 --> 00:43:41,925
uh, the square loss or s- you know,

805
00:43:41,925 --> 00:43:43,439
some other loss. [NOISE]

806
00:43:43,439 --> 00:43:46,725
Um, let's just think about as a kind of abstract function.

807
00:43:46,725 --> 00:43:50,970
So one-dimension, the training loss might look something like this.

808
00:43:50,970 --> 00:43:53,430
You have a single weight and for

809
00:43:53,430 --> 00:43:56,910
each weight you have a number which is your loss on your training samples.

810
00:43:56,910 --> 00:44:00,030
[NOISE] Okay? And you want to find this point.

811
00:44:00,030 --> 00:44:01,935
So in two dimensions,

812
00:44:01,935 --> 00:44:04,650
um, it looks something like this.

813
00:44:04,650 --> 00:44:06,930
Yeah. Let me try and actually draw this because I think it'll,

814
00:44:06,930 --> 00:44:09,030
[NOISE] uh, be, um, useful

815
00:44:09,030 --> 00:44:13,290
in a bit to solve, let me pull this up. [NOISE]

816
00:44:13,290 --> 00:44:15,405
Okay. So in two dimensions,

817
00:44:15,405 --> 00:44:17,850
um, what optimization looks like is as follows.

818
00:44:17,850 --> 00:44:20,865
So I'm gonna- I'm now plotting,

819
00:44:20,865 --> 00:44:27,375
um, W_1 and W_2 which are the two components of this two-dimensional weight vector.

820
00:44:27,375 --> 00:44:29,760
For every point I have a weight vector and

821
00:44:29,760 --> 00:44:33,150
that value is gonna be the loss, the training loss.

822
00:44:33,150 --> 00:44:34,980
Um, and it's, er, you know,

823
00:44:34,980 --> 00:44:39,855
[NOISE] it's pretty standard in these settings to draw what are called level curves.

824
00:44:39,855 --> 00:44:43,785
Um, so let's do this.

825
00:44:43,785 --> 00:44:49,035
So each curve here is a ring of points where,

826
00:44:49,035 --> 00:44:51,720
uh, the function value is identical.

827
00:44:51,720 --> 00:44:54,900
So if you, uh, look at terrain maps, those are level curves.

828
00:44:54,900 --> 00:44:56,910
So you know, kind of what I'm talking about.

829
00:44:56,910 --> 00:45:02,285
So this is the minimum and as you kind of grow out you get larger and larger, um.

830
00:45:02,285 --> 00:45:04,970
Okay. I'll keep on doing this for a little bit.

831
00:45:04,970 --> 00:45:07,390
Okay. [NOISE] All right.

832
00:45:07,390 --> 00:45:11,690
Um. [NOISE] And, uh, the goal is to find the minimum.

833
00:45:11,690 --> 00:45:14,120
Okay. All right.

834
00:45:14,120 --> 00:45:15,410
So how are we gonna do this?

835
00:45:15,410 --> 00:45:17,915
So yeah, question.

836
00:45:17,915 --> 00:45:20,180
Assuming that there is a single minimum.

837
00:45:20,180 --> 00:45:21,755
Yeah, why am I assuming,

838
00:45:21,755 --> 00:45:23,060
uh, there is a single minimum.

839
00:45:23,060 --> 00:45:25,405
[NOISE] in general for arbitrary loss functions,

840
00:45:25,405 --> 00:45:27,795
there is not necessary a single minimum,

841
00:45:27,795 --> 00:45:29,760
I'm just doing this for simplicity.

842
00:45:29,760 --> 00:45:31,950
It turns out to be true for, um,

843
00:45:31,950 --> 00:45:34,890
you know, uh, many of these linear classifiers.

844
00:45:34,890 --> 00:45:39,960
[NOISE]

845
00:45:39,960 --> 00:45:44,745
Okay. So last time we talked about gradient descent, right?

846
00:45:44,745 --> 00:45:48,690
And the idea behind gradient descent is that well, I don't know where this is.

847
00:45:48,690 --> 00:45:49,980
So let's just start at 0,

848
00:45:49,980 --> 00:45:51,540
[NOISE] as good as any place.

849
00:45:51,540 --> 00:45:55,425
And what I'm gonna do at 0 is I'm gonna compute the gradient.

850
00:45:55,425 --> 00:45:58,200
So the gradient is this vector that's,

851
00:45:58,200 --> 00:46:01,200
uh, perpendicular to the level curves.

852
00:46:01,200 --> 00:46:04,620
So the gradient is gonna point in this direction.

853
00:46:04,620 --> 00:46:07,140
That says, hey, in this direction is where

854
00:46:07,140 --> 00:46:10,320
the function is increasing the most dramatically.

855
00:46:10,320 --> 00:46:13,485
Um, and gradient descent says,

856
00:46:13,485 --> 00:46:16,740
um, takes- goes in the opposite direction, right?

857
00:46:16,740 --> 00:46:19,035
Because remember we wanna minimize loss.

858
00:46:19,035 --> 00:46:21,360
Um, so I'm gonna go here.

859
00:46:21,360 --> 00:46:25,845
And, um, now I'll hopefully reduce my, uh,

860
00:46:25,845 --> 00:46:28,470
function value, not necessarily but,

861
00:46:28,470 --> 00:46:30,660
um, we hope that's- that's the case.

862
00:46:30,660 --> 00:46:32,760
Now, we compute, uh, the gradient [NOISE] again.

863
00:46:32,760 --> 00:46:34,365
The gradient says, um,

864
00:46:34,365 --> 00:46:36,765
you know, maybe it's pointing this way.

865
00:46:36,765 --> 00:46:39,360
So I go in that direction and maybe now it's,

866
00:46:39,360 --> 00:46:40,950
uh, pointing this way.

867
00:46:40,950 --> 00:46:42,870
And I keep on going.

868
00:46:42,870 --> 00:46:46,185
Um, this is a little bit made up.

869
00:46:46,185 --> 00:46:49,395
Um, but hopefully, eventually I get to the,

870
00:46:49,395 --> 00:46:52,140
um, the [NOISE] origin.

871
00:46:52,140 --> 00:46:56,175
And you know, I'm, I'm kind of simplifying things quite a bit here.

872
00:46:56,175 --> 00:46:59,580
So in- there's a whole field of optimization that studies exactly what kind of

873
00:46:59,580 --> 00:47:04,200
functions you can optimize and how gradient descent when it works and when it doesn't.

874
00:47:04,200 --> 00:47:07,635
Um, I'm just gonna kind of go through the mechanics now and defer

875
00:47:07,635 --> 00:47:12,850
the kind of the formal proofs of when this actually works until, um, later.

876
00:47:13,490 --> 00:47:18,540
Okay. So that's kind of the- the schema of how gradient descent works.

877
00:47:18,540 --> 00:47:20,010
So in code this looks like this.

878
00:47:20,010 --> 00:47:24,855
So initialize at 0 and then loop in some number of iterations,

879
00:47:24,855 --> 00:47:29,100
um, which let's- for simplicity just think there's a fixed number of iterations.

880
00:47:29,100 --> 00:47:31,500
And then, I'm gonna pick up my weights,

881
00:47:31,500 --> 00:47:34,290
compute the gradient, move in the opposite direction,

882
00:47:34,290 --> 00:47:36,375
and then there's gonna be a step size that, uh,

883
00:47:36,375 --> 00:47:38,280
tells me how fast I want to,

884
00:47:38,280 --> 00:47:39,960
you know, make progress.

885
00:47:39,960 --> 00:47:42,585
Okay? And we'll come back to,

886
00:47:42,585 --> 00:47:43,710
you know, uh, what,

887
00:47:43,710 --> 00:47:48,160
uh, the step size, uh, does later.

888
00:47:49,140 --> 00:47:53,755
Okay. So let's specialize it to a least squares, uh, regression.

889
00:47:53,755 --> 00:47:55,930
So we kind of did this last week,

890
00:47:55,930 --> 00:47:58,615
but, um, just to kind of review, um.

891
00:47:58,615 --> 00:48:04,240
So the training loss for least squares regression is this.

892
00:48:04,240 --> 00:48:09,340
So remember it's an average over the loss of individual examples,

893
00:48:09,340 --> 00:48:12,655
and the loss of a particular example is the residual squared.

894
00:48:12,655 --> 00:48:14,755
So that's this expression.

895
00:48:14,755 --> 00:48:18,265
Um, and then all we have to do is compute the gradient.

896
00:48:18,265 --> 00:48:21,205
And you know, if you remember your calculus,

897
00:48:21,205 --> 00:48:23,875
it's just I've used the chain rule.

898
00:48:23,875 --> 00:48:25,915
So this two comes down here.

899
00:48:25,915 --> 00:48:28,030
You have the, um, you know,

900
00:48:28,030 --> 00:48:31,990
the residual times the derivative of what's inside

901
00:48:31,990 --> 00:48:36,595
here and the gradient with respect to W is, uh, phi of x.

902
00:48:36,595 --> 00:48:42,085
Okay. So last time we did this in Python in 1-dimension.

903
00:48:42,085 --> 00:48:44,830
So 1-dimension, and hopefully all of you should feel comfortable doing

904
00:48:44,830 --> 00:48:48,145
this because this is just kind of basic, um, calculus.

905
00:48:48,145 --> 00:48:52,105
Um, here we have w is a vector.

906
00:48:52,105 --> 00:48:55,975
So, uh, we're not taking derivatives but we're taking gradients.

907
00:48:55,975 --> 00:48:57,930
Um, so there's, you know,

908
00:48:57,930 --> 00:48:59,580
some things to be, uh,

909
00:48:59,580 --> 00:49:04,560
wary of but in this case it's often kind of useful to double-check that.

910
00:49:04,560 --> 00:49:08,475
Well, um, the gradient version actually matches, uh,

911
00:49:08,475 --> 00:49:11,075
the, the single-dimensional version

912
00:49:11,075 --> 00:49:14,470
as well because last time remember we have the x out here.

913
00:49:14,470 --> 00:49:20,275
Um, and one thing to note here is that,

914
00:49:20,275 --> 00:49:25,585
um, there's a prediction minus target, and that's the residual.

915
00:49:25,585 --> 00:49:28,225
So the gradient is driven by,

916
00:49:28,225 --> 00:49:30,865
um, you know, kind of this quantity.

917
00:49:30,865 --> 00:49:35,935
So if the prediction equals the target, uh, what's the gradient?

918
00:49:35,935 --> 00:49:38,575
It's going to be 0 which is kind of what you want.

919
00:49:38,575 --> 00:49:41,965
If you're already getting the answer correct,

920
00:49:41,965 --> 00:49:46,525
then you shouldn't want to move your, uh, your weights, right?

921
00:49:46,525 --> 00:49:51,850
So often you know we can do things in the abstract and everything will work.

922
00:49:51,850 --> 00:49:56,560
But you know it's, it's often a good idea to write down some objective functions,

923
00:49:56,560 --> 00:50:00,100
take the gradient and see if gradient descent on using

924
00:50:00,100 --> 00:50:03,835
these gradients that you computed is kind of a sensible thing because there's

925
00:50:03,835 --> 00:50:08,100
kind of many layers you can understand and get intuition for this stuff at

926
00:50:08,100 --> 00:50:12,690
the kind of abstract level optimization or kind of at the algorithmic level.

927
00:50:12,690 --> 00:50:15,720
Like you pick up an example is it sensible to update when

928
00:50:15,720 --> 00:50:19,870
the gradient other than when the prediction equals the target.

929
00:50:20,330 --> 00:50:26,010
Okay, so so let's take the code that we have from our, from last time,

930
00:50:26,010 --> 00:50:29,310
and I'm going to expand on it a little bit,

931
00:50:29,310 --> 00:50:33,825
and hopefully set the stage for doing stochastic gradient.

932
00:50:33,825 --> 00:50:35,835
Um, okay.

933
00:50:35,835 --> 00:50:40,570
So, so last time we had gradient descent.

934
00:50:40,940 --> 00:50:44,280
Okay, so remember last time we defined a set of points,

935
00:50:44,280 --> 00:50:47,575
we defined the function which is the train loss here.

936
00:50:47,575 --> 00:50:51,940
Um, we defined the derivative of the function, and then we have gradient descent.

937
00:50:51,940 --> 00:50:56,620
Okay, um, so I'm gonna do a little bit of housecleaning and I'm just,

938
00:50:56,620 --> 00:51:01,000
uh, um, don't mind me.

939
00:51:01,000 --> 00:51:04,510
Um, okay so I'm gonna make this a little bit more explicit,

940
00:51:04,510 --> 00:51:06,025
what this algorithm is.

941
00:51:06,025 --> 00:51:09,415
Gradient descent depends on, um, a function,

942
00:51:09,415 --> 00:51:13,750
a derivative of a function and let say, um, you know,

943
00:51:13,750 --> 00:51:19,780
the dimensionality, um, and I can call this gradient FDF

944
00:51:19,780 --> 00:51:24,790
and in this case it's, uh, D where D equals 2.

945
00:51:24,790 --> 00:51:28,675
Okay, and I want to kind of separate.

946
00:51:28,675 --> 00:51:33,985
This is the kind of algorithms and this is, you know, modeling.

947
00:51:33,985 --> 00:51:37,540
So this is what we want to compute and this is,

948
00:51:37,540 --> 00:51:41,050
you know, how we compute it. [NOISE]

949
00:51:41,050 --> 00:51:44,080
Okay and this code should still work. Okay, um.

950
00:51:44,080 --> 00:51:46,630
All right, so what I'm gonna do now is,

951
00:51:46,630 --> 00:51:48,670
um, upgrade this to vector.

952
00:51:48,670 --> 00:51:52,330
So remember the x here is just a number, right?

953
00:51:52,330 --> 00:51:53,890
But we want to support vectors.

954
00:51:53,890 --> 00:51:58,000
Um, so in Python,

955
00:51:58,000 --> 00:52:01,105
um, we're going to import NumPy so which is this, uh,

956
00:52:01,105 --> 00:52:06,710
nice vector and matrix library um, and,

957
00:52:06,900 --> 00:52:09,940
um, I'm gonna make some,

958
00:52:09,940 --> 00:52:11,575
you know, arrays here,

959
00:52:11,575 --> 00:52:15,970
um, which this is just going to be a one-dimensional array.

960
00:52:15,970 --> 00:52:17,440
So it's not that exciting.

961
00:52:17,440 --> 00:52:23,000
So this, this w dot x becomes,

962
00:52:24,540 --> 00:52:27,730
uh, the actual dot I need to call.

963
00:52:27,730 --> 00:52:33,950
And I think w needs to be np.zeros(d).

964
00:52:34,530 --> 00:52:37,750
Okay. All right.

965
00:52:37,750 --> 00:52:40,990
So that's just- should still run actually,

966
00:52:40,990 --> 00:52:43,825
sorry, this is 1-dimensional.

967
00:52:43,825 --> 00:52:46,720
Okay. So remember last time we ran this,

968
00:52:46,720 --> 00:52:49,585
uh, this program and, um,

969
00:52:49,585 --> 00:52:52,480
it starts out with some weights and then it

970
00:52:52,480 --> 00:52:55,975
converges to 0.8 and the function value kind of keeps on going on.

971
00:52:55,975 --> 00:52:58,600
Okay. All right, so let's,

972
00:52:58,600 --> 00:53:01,205
let's try to, um,

973
00:53:01,205 --> 00:53:05,040
you know it's really hard to kind of see you whether

974
00:53:05,040 --> 00:53:07,020
this algorithm is any, doing anything

975
00:53:07,020 --> 00:53:10,235
interesting because we only have two points, it's kind of trivial.

976
00:53:10,235 --> 00:53:13,855
So how do we go about,

977
00:53:13,855 --> 00:53:16,810
um, you know, because I'm going to also implement stochastic gradient descent.

978
00:53:16,810 --> 00:53:20,800
How do we have kind of a test case to see if this algorithm is, you know, working?

979
00:53:20,800 --> 00:53:24,205
Um, so there's kind of this technique which I,

980
00:53:24,205 --> 00:53:27,265
I really like [NOISE] which is to call,

981
00:53:27,265 --> 00:53:32,950
generate artificial data and ideas that, you know, what is learning.

982
00:53:32,950 --> 00:53:37,585
You're learning as you're taking a dataset and you're trying to fit- find the,

983
00:53:37,585 --> 00:53:39,250
the weights that best fit our dataset.

984
00:53:39,250 --> 00:53:42,025
Uh, but in general if I generate some arbitrary,

985
00:53:42,025 --> 00:53:43,990
if I downloaded a dataset I have no idea

986
00:53:43,990 --> 00:53:46,840
what the right kind of quote unquote right answer is.

987
00:53:46,840 --> 00:53:49,090
So there's a technique where I go backwards and say,

988
00:53:49,090 --> 00:53:52,375
okay let's let's decide what the right answer is.

989
00:53:52,375 --> 00:53:55,000
So let's say the right answer is,

990
00:53:55,000 --> 00:53:56,770
um, 1, 2, 3, 4, 5.

991
00:53:56,770 --> 00:53:58,255
So it's a 5-dimensional problem.

992
00:53:58,255 --> 00:54:07,330
Okay. Um, and I'm going to generate some data based on that so that this,

993
00:54:07,330 --> 00:54:10,645
uh, weight vector is kind of good for that data.

994
00:54:10,645 --> 00:54:17,275
Um, I'm going to skip all my breaks in this lecture.

995
00:54:17,275 --> 00:54:21,640
Um, so I'm going to generate a bunch of points.

996
00:54:21,640 --> 00:54:23,140
So let's generate 10,000 point.

997
00:54:23,140 --> 00:54:26,005
The nice thing about artificial data is you can generate as much as you'd want.

998
00:54:26,005 --> 00:54:28,120
Um, there's a question, yeah?

999
00:54:28,120 --> 00:54:30,340
A true w?

1000
00:54:30,340 --> 00:54:33,190
So true w just means like the, the correct,

1001
00:54:33,190 --> 00:54:35,380
the ground truth, the w.

1002
00:54:35,380 --> 00:54:38,770
The true y, true output or?

1003
00:54:38,770 --> 00:54:41,260
So w is a weight vector.

1004
00:54:41,260 --> 00:54:43,120
So this is kind of going backwards.

1005
00:54:43,120 --> 00:54:44,590
Remember, I want to fit the weight vector

1006
00:54:44,590 --> 00:54:48,700
but um, I'm just kind of saying this is the right answer.

1007
00:54:48,700 --> 00:54:52,240
So I want to make sure that the algorithm actually recovers this later.

1008
00:54:52,240 --> 00:54:54,850
Okay, so I'm going to generate some random data.

1009
00:54:54,850 --> 00:54:56,260
So there's a nice function,

1010
00:54:56,260 --> 00:55:03,130
random.randn which generates a random d-dimensional vector and y.

1011
00:55:03,130 --> 00:55:11,043
I'm gonna set- what should I set y to?

1012
00:55:11,043 --> 00:55:12,910
Which side of w you want?

1013
00:55:12,910 --> 00:55:14,890
Yeah. So I'm gonna do regressions.

1014
00:55:14,890 --> 00:55:17,395
So I want to do, uh,

1015
00:55:17,395 --> 00:55:22,825
true_w dot uh, x, right?

1016
00:55:22,825 --> 00:55:26,395
So I mean if you think about it,

1017
00:55:26,395 --> 00:55:29,965
if I took this data and I

1018
00:55:29,965 --> 00:55:33,220
found the, the like true one- w is the right thing

1019
00:55:33,220 --> 00:55:35,725
that we'll get 0 loss here.

1020
00:55:35,725 --> 00:55:39,490
Okay. But I'm going to make your life a little bit

1021
00:55:39,490 --> 00:55:43,690
more interesting and we're gonna add some noise.

1022
00:55:43,690 --> 00:55:49,615
Okay, so let's print out what that looks like.

1023
00:55:49,615 --> 00:55:53,990
Also I should add it to my dataset.

1024
00:55:54,600 --> 00:56:00,010
So okay, so this is my dataset.

1025
00:56:00,010 --> 00:56:03,505
Okay, I mean, I can't really tell what's going on but,

1026
00:56:03,505 --> 00:56:07,660
but you can look at the code and you, you can assure yourself that,

1027
00:56:07,660 --> 00:56:11,770
uh, this data has structure in it. [NOISE]

1028
00:56:11,770 --> 00:56:17,320
Okay, so let's get rid of this print statement and let's train and see what happens.

1029
00:56:17,320 --> 00:56:20,065
So let's.

1030
00:56:20,065 --> 00:56:24,070
Okay. Oh, one thing I forgot to do.

1031
00:56:24,070 --> 00:56:28,450
Um, so if you notice that the objective functions that I've, uh,

1032
00:56:28,450 --> 00:56:32,530
written down they haven't divided by the number of data points.

1033
00:56:32,530 --> 00:56:35,890
I want the average loss, not the, the sum.

1034
00:56:35,890 --> 00:56:38,380
Um, it turns out that, you know if you have the sum,

1035
00:56:38,380 --> 00:56:40,750
then things get really big and you know, blow up.

1036
00:56:40,750 --> 00:56:43,120
So let me just normalize that.

1037
00:56:43,120 --> 00:56:48,965
Okay. So let me lock it.

1038
00:56:48,965 --> 00:56:51,885
Okay, so it's training, it's training.

1039
00:56:51,885 --> 00:56:53,580
Um, actually so let me,

1040
00:56:53,580 --> 00:56:54,930
uh, do more iterations.

1041
00:56:54,930 --> 00:56:58,645
So I did 100 iterations, let's do 1000 iterations.

1042
00:56:58,645 --> 00:57:02,875
Okay. So when the function value is going down,

1043
00:57:02,875 --> 00:57:04,870
that's always something to- you know, good to check.

1044
00:57:04,870 --> 00:57:09,565
Um, and you can see the weights are kind of slowly getting to,

1045
00:57:09,565 --> 00:57:10,900
you know, what appears to be 1,

1046
00:57:10,900 --> 00:57:13,945
2, 3, 4, 5, right?

1047
00:57:13,945 --> 00:57:17,800
Okay. So this is a hard proof but it's kind of

1048
00:57:17,800 --> 00:57:22,310
evidence that this learning algorithm is actually kind of doing the right thing.

1049
00:57:23,010 --> 00:57:27,820
Um, okay so now let's see if I add,

1050
00:57:27,820 --> 00:57:29,620
you know more points.

1051
00:57:29,620 --> 00:57:32,545
So I now have 100,000 points.

1052
00:57:32,545 --> 00:57:36,400
Now, you know, obviously it gets slower,

1053
00:57:36,400 --> 00:57:38,950
um, and you'll, you know, hopefully get there you know,

1054
00:57:38,950 --> 00:57:41,360
one day but I'm just gonna kill it.

1055
00:57:42,060 --> 00:57:44,905
Okay, any questions about,

1056
00:57:44,905 --> 00:57:47,575
uh, oops, my terminal got screwed up.

1057
00:57:47,575 --> 00:57:50,245
Okay. So what did I do here,

1058
00:57:50,245 --> 00:57:53,620
I defined loss functions, took their derivatives.

1059
00:57:53,620 --> 00:57:59,140
Um, the gradient descent is what we implemented last time and the only thing different

1060
00:57:59,140 --> 00:58:01,540
I did, this time is generated data sets so I can kind

1061
00:58:01,540 --> 00:58:04,750
of check whether gradient descent is working. Yeah question.

1062
00:58:04,750 --> 00:58:11,000
So the fact that the gradient is just the residual [inaudible]

1063
00:58:11,000 --> 00:58:16,600
a algorithm to learn from overpredictions versus like underpredictions?

1064
00:58:16,600 --> 00:58:19,960
The question is whether the fact that the gradient is residual

1065
00:58:19,960 --> 00:58:23,110
allows the algorithm to learn from under or over predictions.

1066
00:58:23,110 --> 00:58:27,430
Um, yeah. So the gradient is if you think about it,

1067
00:58:27,430 --> 00:58:28,540
yeah that's good intuition.

1068
00:58:28,540 --> 00:58:30,835
So if you look at, um,

1069
00:58:30,835 --> 00:58:33,595
if you're over-predicting, right?

1070
00:58:33,595 --> 00:58:37,705
That means the gradient is kind of- assume that this is like 1.

1071
00:58:37,705 --> 00:58:41,425
So that means this is going to be positive which means that, hey if you opt that way,

1072
00:58:41,425 --> 00:58:43,900
you're going to over-predict more and more and incur more loss.

1073
00:58:43,900 --> 00:58:46,555
So, um, by subtracting a gradient,

1074
00:58:46,555 --> 00:58:48,910
you're kind of pushing the weights out in

1075
00:58:48,910 --> 00:58:52,585
the other direction and same for when you're, um, you're under-predicting.

1076
00:58:52,585 --> 00:58:54,280
Yeah, so that's good intuition to have.

1077
00:58:54,280 --> 00:58:57,310
Yeah.

1078
00:58:57,310 --> 00:59:03,425
What is the effect of the noise when you generate [inaudible]

1079
00:59:03,425 --> 00:59:06,050
What is the effect of the noise?

1080
00:59:06,050 --> 00:59:07,280
Um, the effect of the noise,

1081
00:59:07,280 --> 00:59:09,590
it makes the problem a little bit, you know,

1082
00:59:09,590 --> 00:59:13,580
harder so that it takes more examples to learn.

1083
00:59:13,580 --> 00:59:17,750
Um, if you shut off the noise then it will- you know, we can try that.

1084
00:59:17,750 --> 00:59:19,490
Um, I've never done this before,

1085
00:59:19,490 --> 00:59:21,410
but presumably you'll learn, you know,

1086
00:59:21,410 --> 00:59:24,065
f- faster, but maybe not.

1087
00:59:24,065 --> 00:59:26,870
Um, the noise isn't, you know,

1088
00:59:26,870 --> 00:59:33,200
that much. But, um, okay.

1089
00:59:33,200 --> 00:59:35,930
So, so let's say you have,

1090
00:59:35,930 --> 00:59:39,245
you know, like 500 examp- 1000 examples.

1091
00:59:39,245 --> 00:59:40,990
You know, that's quite a few examples.

1092
00:59:40,990 --> 00:59:43,090
As in now, you know, this algorithm runs,

1093
00:59:43,090 --> 00:59:44,350
you know, pretty slowly, right?

1094
00:59:44,350 --> 00:59:46,375
And in- in modern machine learning you have,

1095
00:59:46,375 --> 00:59:48,700
you know, millions or hundreds of millions of examples.

1096
00:59:48,700 --> 00:59:53,205
So gradient descent is gonna be, you know, pretty slow.

1097
00:59:53,205 --> 00:59:58,085
So how can we speed things up a little bit,

1098
00:59:58,085 --> 01:00:00,215
and what's the problem here?

1099
01:00:00,215 --> 01:00:06,110
Well, if you look at the- the- what the algorithm is doing, it's iterating.

1100
01:00:06,110 --> 01:00:09,530
And each iteration it's computing the gradient of the training loss.

1101
01:00:09,530 --> 01:00:11,585
And the training loss is,

1102
01:00:11,585 --> 01:00:13,010
um, average of all the points,

1103
01:00:13,010 --> 01:00:15,440
which means that you have to go through all the points and you

1104
01:00:15,440 --> 01:00:18,800
compute the lo- gradient of the loss and you add everything up.

1105
01:00:18,800 --> 01:00:23,120
And that's what is expensive and, you know, it takes time.

1106
01:00:23,120 --> 01:00:26,030
So, you know, you might wonder,

1107
01:00:26,030 --> 01:00:27,980
well, how, how can you avoid this?

1108
01:00:27,980 --> 01:00:31,625
I mean, you- if you wanted to do gradient descent you have to go through all your points.

1109
01:00:31,625 --> 01:00:37,430
Um, and the, the key insight behind stochastic gradient descent is that,

1110
01:00:37,430 --> 01:00:40,625
well maybe- maybe you don't have to do that.

1111
01:00:40,625 --> 01:00:43,325
So, um, maybe- you know,

1112
01:00:43,325 --> 01:00:45,815
here- here's some intuition, right?

1113
01:00:45,815 --> 01:00:48,575
So what is- what is this gradient?

1114
01:00:48,575 --> 01:00:51,395
So this gradient is actually the sum of

1115
01:00:51,395 --> 01:00:55,025
all the gradients from all the examples in your training set.

1116
01:00:55,025 --> 01:00:58,580
Right? So we have 500,000 points adding to that.

1117
01:00:58,580 --> 01:01:01,925
So actually what this gradient is- is, um,

1118
01:01:01,925 --> 01:01:05,510
it's actually kind of a sum of different things which are maybe

1119
01:01:05,510 --> 01:01:09,755
pointing in slightly different directions which all average out to this direction.

1120
01:01:09,755 --> 01:01:17,390
Okay. So maybe you can actually not average all of them,

1121
01:01:17,390 --> 01:01:19,145
but you can, um,

1122
01:01:19,145 --> 01:01:21,710
average just a couple or maybe even in an

1123
01:01:21,710 --> 01:01:24,725
extreme case you can just like take one of them and just,

1124
01:01:24,725 --> 01:01:26,555
you know, march in that direction.

1125
01:01:26,555 --> 01:01:29,420
So, so here's the idea behind stochastic gradient descent.

1126
01:01:29,420 --> 01:01:31,145
So instead of doing gradient descent,

1127
01:01:31,145 --> 01:01:36,755
we are going to change the algorithm to say for each example in the training set,

1128
01:01:36,755 --> 01:01:41,165
I'm just going to pick it up and just update, you know.

1129
01:01:41,165 --> 01:01:43,460
It's- instead of like sitting down and

1130
01:01:43,460 --> 01:01:45,830
looking at all of the training examples and thinking really hard,

1131
01:01:45,830 --> 01:01:48,785
I'm just gonna pick up one training example and update right away.

1132
01:01:48,785 --> 01:01:50,630
So again, the key idea here is,

1133
01:01:50,630 --> 01:01:53,795
it's not about quality it's about, uh, quantity.

1134
01:01:53,795 --> 01:01:56,270
May be not the world's best life lesson,

1135
01:01:56,270 --> 01:02:00,300
but it seems to work in- it works in here.

1136
01:02:00,850 --> 01:02:06,140
Um, and then, there's also this question of what should the step size be?

1137
01:02:06,140 --> 01:02:09,320
And in- generally, in stochastic gradient descent,

1138
01:02:09,320 --> 01:02:12,605
it's actually even a bit more important because,

1139
01:02:12,605 --> 01:02:16,235
um, when you're updating on each- each individual example,

1140
01:02:16,235 --> 01:02:19,310
you're getting kind of noisy estimates of the actual gradient.

1141
01:02:19,310 --> 01:02:22,205
And, uh, and people often ask me like,

1142
01:02:22,205 --> 01:02:24,095
"Oh, how should I set my step size and all."

1143
01:02:24,095 --> 01:02:27,425
And the answer is like there is no formula.

1144
01:02:27,425 --> 01:02:29,180
I mean, there are formulas,

1145
01:02:29,180 --> 01:02:31,625
but there's no kind of definitive answer.

1146
01:02:31,625 --> 01:02:33,575
Here's some general guidance.

1147
01:02:33,575 --> 01:02:35,990
Um, so if step size is small,

1148
01:02:35,990 --> 01:02:37,655
so really close to 0,

1149
01:02:37,655 --> 01:02:40,355
that means you are taking tiny steps, right?

1150
01:02:40,355 --> 01:02:44,315
That means that it'll take longer to get where you want to go,

1151
01:02:44,315 --> 01:02:46,400
but you're kind of proceeding cautiously.

1152
01:02:46,400 --> 01:02:49,250
so it's less likely you're gonna,

1153
01:02:49,250 --> 01:02:51,440
you know- uh, if you mess up and go in

1154
01:02:51,440 --> 01:02:54,050
the wrong direction you're not gonna go too far in the wrong direction.

1155
01:02:54,050 --> 01:02:57,800
Um, conversely, if you have it to be really,

1156
01:02:57,800 --> 01:03:01,490
really, large then, you know, it's like a race car.

1157
01:03:01,490 --> 01:03:03,050
You, kind of, drive really fast,

1158
01:03:03,050 --> 01:03:06,815
but you might just kind of bounce around a lot.

1159
01:03:06,815 --> 01:03:10,145
So, pictorially what this looks like is that, you know,

1160
01:03:10,145 --> 01:03:11,690
here's maybe a moderate step size,

1161
01:03:11,690 --> 01:03:13,625
but if you're taking steps,

1162
01:03:13,625 --> 01:03:15,785
really big steps, um,

1163
01:03:15,785 --> 01:03:18,680
you might go over here and then you jump around

1164
01:03:18,680 --> 01:03:21,155
and then maybe, maybe you'll end up in the right place but maybe

1165
01:03:21,155 --> 01:03:24,365
sometimes you can actually get flung off out of orbit

1166
01:03:24,365 --> 01:03:29,120
and diverge to infinity which is a bad situation.

1167
01:03:29,120 --> 01:03:33,170
Um, so there's many ways to set the step size.

1168
01:03:33,170 --> 01:03:35,270
You can set it to a, you know, constant.

1169
01:03:35,270 --> 01:03:36,980
You can- usually, you have to,

1170
01:03:36,980 --> 01:03:38,705
um, you know, tune it.

1171
01:03:38,705 --> 01:03:42,320
Or you can set it to be decreasing the intuition

1172
01:03:42,320 --> 01:03:45,605
being that as you optimize and get closer to the optimum,

1173
01:03:45,605 --> 01:03:47,210
you kind of want to slow down, right?

1174
01:03:47,210 --> 01:03:49,970
Like if you- you're coming on the freeway, you're driving really fast,

1175
01:03:49,970 --> 01:03:51,530
but once you get to your house you probably

1176
01:03:51,530 --> 01:03:54,720
don't want to be like driving 60 miles an hour.

1177
01:03:55,000 --> 01:04:00,380
Okay. So- actually I didn't implement stochastic gradient.

1178
01:04:00,380 --> 01:04:06,060
So let me do that. So let's, let's try to get stochastic gradient up and going here.

1179
01:04:06,880 --> 01:04:12,350
Okay. So, so the interface to stochastic gradient changes.

1180
01:04:12,350 --> 01:04:15,995
So- right? So the- in gradients then all you need is a function.

1181
01:04:15,995 --> 01:04:21,180
And it just kind of computes the sum over all the training examples.

1182
01:04:21,180 --> 01:04:24,155
Um, so in stochastic gradient,

1183
01:04:24,155 --> 01:04:27,140
I'm just going to denote S as for stochastic gradient.

1184
01:04:27,140 --> 01:04:29,570
I'm gonna take an index I,

1185
01:04:29,570 --> 01:04:32,840
and I'm going to update on the Ith point only.

1186
01:04:32,840 --> 01:04:35,405
So I'm going to only compute the loss on the Ith point.

1187
01:04:35,405 --> 01:04:37,715
And same for its derivative.

1188
01:04:37,715 --> 01:04:41,435
Um, you can look at the Ith point,

1189
01:04:41,435 --> 01:04:46,910
um, and just compute the gradient on that Ith point.

1190
01:04:46,910 --> 01:04:51,755
Okay? And this should be called SDF.

1191
01:04:51,755 --> 01:04:55,175
Okay. So now instead of doing gradient descent,

1192
01:04:55,175 --> 01:04:58,070
let's do stochastic gradient descent.

1193
01:04:58,070 --> 01:05:03,140
And I'm going to pass in sf, sdf, d, and,

1194
01:05:03,140 --> 01:05:07,505
um, the number of points because I need to know how many points there are now.

1195
01:05:07,505 --> 01:05:10,100
Um, copy gradient descent,

1196
01:05:10,100 --> 01:05:11,570
and it's basically kind of the same function.

1197
01:05:11,570 --> 01:05:13,625
I'm just going to stick another for loop there.

1198
01:05:13,625 --> 01:05:15,829
So stochastic gradient descent,

1199
01:05:15,829 --> 01:05:18,529
it's going to take the stochastic functions,

1200
01:05:18,529 --> 01:05:23,180
stochastic gradient, the dimensionality and- Okay?

1201
01:05:23,180 --> 01:05:26,735
So now, before I was just going through, um,

1202
01:05:26,735 --> 01:05:30,245
number of iterations and now, right,

1203
01:05:30,245 --> 01:05:33,860
I'm not going to try to compute the value of the- all the training examples.

1204
01:05:33,860 --> 01:05:35,990
I'm going to, um,

1205
01:05:35,990 --> 01:05:38,435
loop over all the points

1206
01:05:38,435 --> 01:05:45,680
and I'm going to call just evaluate the function at that point I,

1207
01:05:45,680 --> 01:05:50,675
and compute the gradient at that point I instead of the entire, you know, dataset.

1208
01:05:50,675 --> 01:05:53,015
And then everything else is the same.

1209
01:05:53,015 --> 01:05:57,440
I mean, one other thing I'll do here is that I'll use a different step size schedule.

1210
01:05:57,440 --> 01:06:03,060
So um, 1 divided by number of updates.

1211
01:06:03,370 --> 01:06:07,340
So I want it so that the number of,

1212
01:06:07,340 --> 01:06:10,950
uh, the step size is gonna decrease over time.

1213
01:06:13,150 --> 01:06:17,540
Okay, so I start with a equals 1 and then it's half,

1214
01:06:17,540 --> 01:06:20,570
and then it's a third, and it's a fourth, and it keeps on going down.

1215
01:06:20,570 --> 01:06:25,040
Um, sometimes you can put a square root and that's more typical in some cases,

1216
01:06:25,040 --> 01:06:29,580
but, um, I'm not going to worry about the details too much. Uh, question?

1217
01:06:30,190 --> 01:06:35,570
The point I is the chosen randomly but here we just [inaudible].

1218
01:06:35,570 --> 01:06:37,850
Yes. The question is- the word

1219
01:06:37,850 --> 01:06:40,655
stochastic means that there should be some randomness here.

1220
01:06:40,655 --> 01:06:43,370
And, you know, technically speaking,

1221
01:06:43,370 --> 01:06:46,220
the- the stochastic gradient descent is where

1222
01:06:46,220 --> 01:06:49,385
you're sampling a random point and then you're updating on it.

1223
01:06:49,385 --> 01:06:51,155
I'm cheating a little bit,

1224
01:06:51,155 --> 01:06:55,700
um, uh, because I'm iterating over all the points.

1225
01:06:55,700 --> 01:06:58,040
You know, in practice if you have a lot of points and you

1226
01:06:58,040 --> 01:07:00,770
randomize the order it's kind of- it's- it's

1227
01:07:00,770 --> 01:07:06,900
similar but it's- there is a kind of a technical difference that I'm trying to hide.

1228
01:07:07,720 --> 01:07:11,495
Okay. So- so this is stochastic gradient descent.

1229
01:07:11,495 --> 01:07:14,555
Um, to iterate, you know,

1230
01:07:14,555 --> 01:07:17,540
go over all the points and just, you know update.

1231
01:07:17,540 --> 01:07:22,580
Okay? Um, so let's see if this works.

1232
01:07:22,580 --> 01:07:25,620
Um, okay.

1233
01:07:29,470 --> 01:07:32,400
I don't think that worked.

1234
01:07:32,400 --> 01:07:34,385
[LAUGHTER]

1235
01:07:34,385 --> 01:07:38,825
Maybe- let's see what happened here?

1236
01:07:38,825 --> 01:07:42,260
I did try it on 100,000 points. Maybe that works.

1237
01:07:42,260 --> 01:07:45,120
And, nope, doesn't work either.

1238
01:07:49,570 --> 01:07:57,620
Um, anyone see the problem?

1239
01:07:57,620 --> 01:08:05,150
[inaudible]

1240
01:08:05,150 --> 01:08:08,060
So I'm printing this, um, out, uh,

1241
01:08:08,060 --> 01:08:12,200
at the- at the end,

1242
01:08:12,200 --> 01:08:14,300
um, of each iteration.

1243
01:08:14,300 --> 01:08:18,000
So that should be fine, um.

1244
01:08:18,000 --> 01:08:19,001
Really, this should work.

1245
01:08:28,420 --> 01:08:31,820
So gradient descent was working, right?

1246
01:08:31,820 --> 01:08:33,260
Maybe I'll, I'll try-

1247
01:08:33,260 --> 01:08:37,730
It's probably not the best idea to be debugging this live.

1248
01:08:37,730 --> 01:08:41,360
Okay. Let's, let's make sure gradient descent works.

1249
01:08:41,360 --> 01:08:45,500
Um, okay, so that was working right.

1250
01:08:45,500 --> 01:08:49,560
Okay. So stochastic gradient descent.

1251
01:08:49,810 --> 01:08:53,180
I mean, it's really fast and converges,

1252
01:08:53,180 --> 01:08:54,320
[LAUGHTER]

1253
01:08:54,320 --> 01:08:56,940
but it doesn't converge to the right answer.

1254
01:08:58,750 --> 01:09:02,930
I think [inaudible].

1255
01:09:02,930 --> 01:09:05,330
Yeah, but that should get incremented to 1.

1256
01:09:05,330 --> 01:09:10,760
So that-

1257
01:09:10,760 --> 01:09:17,470
It might be true.

1258
01:09:17,470 --> 01:09:20,140
Okay, so I do have a version of this code that does work.

1259
01:09:20,140 --> 01:09:20,829
[LAUGHTER]

1260
01:09:20,829 --> 01:09:23,310
So what am I doing here, that's different.

1261
01:09:23,310 --> 01:09:25,280
Okay, I'll have some water. Maybe I need some water.

1262
01:09:25,280 --> 01:09:29,600
[LAUGHTER]

1263
01:09:29,600 --> 01:09:35,000
Okay, so this version works. Yeah.

1264
01:09:35,000 --> 01:09:40,000
[inaudible]

1265
01:09:40,000 --> 01:09:41,675
Yeah, that's- that's probably good.

1266
01:09:41,675 --> 01:09:46,170
That's a good call. Yeah. okay.

1267
01:09:47,260 --> 01:09:49,820
All right. Now, it works. Thank you.

1268
01:09:49,820 --> 01:09:52,000
[LAUGHTER]

1269
01:09:53,710 --> 01:09:56,015
Um, so yeah.

1270
01:09:56,015 --> 01:09:58,205
Yeah, this is a good lesson.

1271
01:09:58,205 --> 01:10:01,655
Um, it's that when you're dividing, um,

1272
01:10:01,655 --> 01:10:04,070
these needs to be one- actually in Python 3,

1273
01:10:04,070 --> 01:10:07,175
this is not a problem but I'm so- on Python 2 for some reason.

1274
01:10:07,175 --> 01:10:10,340
But this should be, uh, 1.0 divided by numUpdates.

1275
01:10:10,340 --> 01:10:12,095
Otherwise, I was getting-

1276
01:10:12,095 --> 01:10:13,340
So how is it faster?

1277
01:10:13,340 --> 01:10:16,550
Okay. So why is it faster?

1278
01:10:16,550 --> 01:10:17,060
[LAUGHTER].

1279
01:10:17,060 --> 01:10:18,890
Yeah, okay.

1280
01:10:18,890 --> 01:10:21,605
Okay. Let's- let's,

1281
01:10:21,605 --> 01:10:24,930
uh, go back to 500,000, okay.

1282
01:10:26,130 --> 01:10:31,930
Okay. So one full sweep over the data is the same amount of time.

1283
01:10:31,930 --> 01:10:34,300
But you notice that immediately,

1284
01:10:34,300 --> 01:10:36,580
it already converges to 1, 2,

1285
01:10:36,580 --> 01:10:39,435
3, 4, 5, right?

1286
01:10:39,435 --> 01:10:42,530
So this is like way, way faster than gradient descent.

1287
01:10:42,530 --> 01:10:44,480
Remember, I just, uh, kind of compare it.

1288
01:10:44,480 --> 01:10:47,615
Um, gradient descent is,

1289
01:10:47,615 --> 01:10:50,540
um, you run it.

1290
01:10:50,540 --> 01:10:52,550
And after one stop, it's, like,

1291
01:10:52,550 --> 01:10:54,630
not even close.

1292
01:10:54,850 --> 01:10:57,020
Right. Yeah?

1293
01:10:57,020 --> 01:11:01,055
What noise levels you have to have until gradient descent becomes better?

1294
01:11:01,055 --> 01:11:05,165
What noise levels you have to have until gradient descent becomes better?

1295
01:11:05,165 --> 01:11:09,950
Um, so it is true that if you have more noise,

1296
01:11:09,950 --> 01:11:11,480
then gradient descent might be, uh,

1297
01:11:11,480 --> 01:11:13,580
stochastic gradient descent can be unstable.

1298
01:11:13,580 --> 01:11:17,030
Um, there might be ways to mitigate that with step size choices.

1299
01:11:17,030 --> 01:11:19,430
But, um, yeah, probably,

1300
01:11:19,430 --> 01:11:23,750
you have to add a lot of noise for stochastic gradient to be, um, really bad.

1301
01:11:23,750 --> 01:11:25,805
Um, I mean, this is in some sense, you know,

1302
01:11:25,805 --> 01:11:29,165
if you take a step back and think about what's going on in this problem,

1303
01:11:29,165 --> 01:11:30,500
it's a 5-dimensional problem.

1304
01:11:30,500 --> 01:11:36,755
There's only five numbers and I'm feeding it half a million data points, right?

1305
01:11:36,755 --> 01:11:40,565
There, there aren't- there's not that much to learn here.

1306
01:11:40,565 --> 01:11:43,160
And so there's a lot of redundancy in the dataset.

1307
01:11:43,160 --> 01:11:44,630
And generally, actually, this is true.

1308
01:11:44,630 --> 01:11:45,830
I go into a large dataset,

1309
01:11:45,830 --> 01:11:47,975
there's gonna be a lot of, you know, redundancy.

1310
01:11:47,975 --> 01:11:52,940
So, uh, going through all of the data and then try to make an informed decision is,

1311
01:11:52,940 --> 01:11:55,280
you know, pretty wasteful, where sometimes you can

1312
01:11:55,280 --> 01:11:57,880
just kind of get a representative sample from, um,

1313
01:11:57,880 --> 01:12:00,460
one example or more as common to do the

1314
01:12:00,460 --> 01:12:03,250
like of kind of mini-batches where you maybe grab a hundred examples

1315
01:12:03,250 --> 01:12:05,830
and you update on that which is- so there's

1316
01:12:05,830 --> 01:12:09,205
a way to be somewhere in between stochastic gradient and gradient descent.

1317
01:12:09,205 --> 01:12:11,160
Okay, let me move on. Um.

1318
01:12:11,160 --> 01:12:12,470
Okay.

1319
01:12:12,470 --> 01:12:15,065
Summary so far, we have linear predictors,

1320
01:12:15,065 --> 01:12:18,470
um, which are based on scores.

1321
01:12:18,470 --> 01:12:22,160
So linear predictors we include both classifiers and regressors, um,

1322
01:12:22,160 --> 01:12:23,900
we can do loss minimization,

1323
01:12:23,900 --> 01:12:25,145
and we can, uh,

1324
01:12:25,145 --> 01:12:26,330
if we implement it correctly,

1325
01:12:26,330 --> 01:12:28,445
we can do, uh, SGD.

1326
01:12:28,445 --> 01:12:33,650
Okay. So that was- I'm kind of switching things.

1327
01:12:33,650 --> 01:12:35,090
I hope you are kind of following along.

1328
01:12:35,090 --> 01:12:37,670
I'll introduced binary classification and

1329
01:12:37,670 --> 01:12:40,970
then, I did all the optimization for linear regression.

1330
01:12:40,970 --> 01:12:43,595
So now, let's go back to classification

1331
01:12:43,595 --> 01:12:46,820
and see if we could do stochastic gradient descent here.

1332
01:12:46,820 --> 01:12:48,980
Okay. So for classification, remember,

1333
01:12:48,980 --> 01:12:52,565
we decided that the zero-one loss is the thing we want.

1334
01:12:52,565 --> 01:12:54,275
We want to minimize the number of mistakes.

1335
01:12:54,275 --> 01:12:55,940
You know, who can argue with that?

1336
01:12:55,940 --> 01:12:59,765
Um, so rem- remember, what is zero-one loss look like? It looks like this.

1337
01:12:59,765 --> 01:13:04,440
Okay? So what happens if I try to run stochastic gradient descent on this?

1338
01:13:06,340 --> 01:13:09,950
Um, I mean, I can run the code,

1339
01:13:09,950 --> 01:13:13,985
but [OVERLAPPING] yeah, it's- it won't work,

1340
01:13:13,985 --> 01:13:15,350
right? And why won't it work?

1341
01:13:15,350 --> 01:13:18,770
[inaudible].

1342
01:13:18,770 --> 01:13:21,695
Yeah. So two popular answers are it's not differentiable,

1343
01:13:21,695 --> 01:13:23,510
that's- it's one problem.

1344
01:13:23,510 --> 01:13:27,530
Um, but I think that the- the bigger problem and kind of deeper problem is that,

1345
01:13:27,530 --> 01:13:29,510
what is the- what is the gradient?

1346
01:13:29,510 --> 01:13:30,230
Zero.

1347
01:13:30,230 --> 01:13:33,170
Zero. It's like zero, basically everywhere except for this point,

1348
01:13:33,170 --> 01:13:34,280
which are, you know, it doesn't really matter.

1349
01:13:34,280 --> 01:13:40,640
So, um, so as- as we learned that if you try to update with a gradient of 0,

1350
01:13:40,640 --> 01:13:43,970
um, then you, you won't move your weights, right?

1351
01:13:43,970 --> 01:13:47,855
So gradient descent will not work on the zero-one, uh, loss.

1352
01:13:47,855 --> 01:13:51,530
Um, so that's- that's kind of unfortunate.

1353
01:13:51,530 --> 01:13:57,590
So how should we fix this problem? Yeah?

1354
01:13:57,590 --> 01:13:59,360
[inaudible]

1355
01:13:59,360 --> 01:14:02,495
Yeah, let's, let's make the gradient non-zero. Let's skew things.

1356
01:14:02,495 --> 01:14:06,680
Um, so there's one loss,

1357
01:14:06,680 --> 01:14:08,825
which I'm gonna introduce called the hinge loss,

1358
01:14:08,825 --> 01:14:11,165
which, uh, does exactly that.

1359
01:14:11,165 --> 01:14:14,015
Um, so let me write the hinge loss down.

1360
01:14:14,015 --> 01:14:16,055
And the hinge loss,

1361
01:14:16,055 --> 01:14:18,920
um, is basically, uh,

1362
01:14:18,920 --> 01:14:24,290
is zero here when the margin is greater than or equal to 1 and rises linearly.

1363
01:14:24,290 --> 01:14:27,110
So if you've gotten it correct by a margin of

1364
01:14:27,110 --> 01:14:30,665
1 so you're kind of pretty safely on the err side of,

1365
01:14:30,665 --> 01:14:33,785
um, getting it correct, then we won't charge you anything.

1366
01:14:33,785 --> 01:14:35,435
But as soon as you start,

1367
01:14:35,435 --> 01:14:37,160
you know, dip into this area,

1368
01:14:37,160 --> 01:14:41,430
we're gonna charge you a kind of a linear amount and your loss is gonna grow linearly.

1369
01:14:41,430 --> 01:14:44,780
Um, so there's some reasons why this is a good idea.

1370
01:14:44,780 --> 01:14:49,190
So it upper bounds the zero-one loss, um, it's, uh,

1371
01:14:49,190 --> 01:14:51,500
it has a property called- known as convexity,

1372
01:14:51,500 --> 01:14:53,810
which means that if you actually run the gradient descent,

1373
01:14:53,810 --> 01:14:56,255
you're actually gonna converge to the global optimum.

1374
01:14:56,255 --> 01:14:58,670
Um, I'm not gonna get into that.

1375
01:14:58,670 --> 01:15:01,460
And so that's, you know,

1376
01:15:01,460 --> 01:15:02,675
that's a hinge loss.

1377
01:15:02,675 --> 01:15:08,555
Um, so what remains to be done is to compute the gradient of this,

1378
01:15:08,555 --> 01:15:10,385
you know, hinge loss, okay?

1379
01:15:10,385 --> 01:15:13,205
So how do you compute this gradient?

1380
01:15:13,205 --> 01:15:16,640
So in some sense, it's a trick question because

1381
01:15:16,640 --> 01:15:19,880
the gradient doesn't exist because it's not,

1382
01:15:19,880 --> 01:15:21,710
um, you know, differentiable everywhere,

1383
01:15:21,710 --> 01:15:26,255
but we're gonna pre- pretend that little point doesn't exist, okay?

1384
01:15:26,255 --> 01:15:29,255
So, so what is this hinge loss?

1385
01:15:29,255 --> 01:15:32,510
The hinge loss is actually two functions, right?

1386
01:15:32,510 --> 01:15:36,140
There is a zero function here and then there's like this,

1387
01:15:36,140 --> 01:15:38,525
uh, 1 minus x function.

1388
01:15:38,525 --> 01:15:40,010
So what am I plotting here?

1389
01:15:40,010 --> 01:15:44,195
I'm plotting the- the margin and, uh, the loss.

1390
01:15:44,195 --> 01:15:45,605
Okay? So this is,

1391
01:15:45,605 --> 01:15:46,970
uh, the zero function,

1392
01:15:46,970 --> 01:15:48,170
and this is, uh,

1393
01:15:48,170 --> 01:15:51,920
1 minus, uh, w dot phi of xy.

1394
01:15:51,920 --> 01:15:55,835
And the hinge loss is just the maxima of these two functions.

1395
01:15:55,835 --> 01:15:56,930
So at every point,

1396
01:15:56,930 --> 01:15:58,565
I'm just taking the top function.

1397
01:15:58,565 --> 01:16:01,865
So um, that's how I am able to trace out,

1398
01:16:01,865 --> 01:16:03,380
uh, this- this curve.

1399
01:16:03,380 --> 01:16:05,975
Okay? All right.

1400
01:16:05,975 --> 01:16:09,500
So if I want to take the gradient of this function,

1401
01:16:09,500 --> 01:16:11,480
you know, you, you can try to do the math.

1402
01:16:11,480 --> 01:16:13,325
Well, let's think through it. You know,

1403
01:16:13,325 --> 01:16:15,500
what- what should the gradient be?

1404
01:16:15,500 --> 01:16:17,540
Um, we're, we're here,

1405
01:16:17,540 --> 01:16:20,075
what should the gradient be? It's zero.

1406
01:16:20,075 --> 01:16:22,640
And if I'm here, what should the gradient be?

1407
01:16:22,640 --> 01:16:26,495
It should be the- whatever the gradient of this function is, right?

1408
01:16:26,495 --> 01:16:29,960
So in general, when you have a gradient of this- of this kind of max,

1409
01:16:29,960 --> 01:16:32,780
uh, you have to kind of break it up into cases.

1410
01:16:32,780 --> 01:16:35,255
Um, and depending on where you are,

1411
01:16:35,255 --> 01:16:38,405
um, you, you have a different case.

1412
01:16:38,405 --> 01:16:43,160
So loss is equal to- if I'm over here,

1413
01:16:43,160 --> 01:16:45,990
and what's the condition for being over here?

1414
01:16:47,440 --> 01:16:56,495
If the margin is greater than 1, right?

1415
01:16:56,495 --> 01:17:01,925
And then otherwise, I'm going to take the gradient of this with respect to w,

1416
01:17:01,925 --> 01:17:06,995
which is gonna be minus phi of x y, you know, otherwise.

1417
01:17:06,995 --> 01:17:11,570
Okay? Um, so again,

1418
01:17:11,570 --> 01:17:15,290
we can try to interpret the, the gradient of the hinge loss.

1419
01:17:15,290 --> 01:17:18,440
So remember your stochastic gradient descent, you have a weight vector,

1420
01:17:18,440 --> 01:17:20,555
and you're gonna pick up an example and you say,

1421
01:17:20,555 --> 01:17:22,670
Oh, let's compute the gradient move away from it.

1422
01:17:22,670 --> 01:17:25,385
So if you're getting the example right,

1423
01:17:25,385 --> 01:17:29,180
then the gradient zero don't move, which is the right thing to do.

1424
01:17:29,180 --> 01:17:33,650
And otherwise, you're going to move in that direction because you're minus,

1425
01:17:33,650 --> 01:17:36,230
minus of phi of x y,

1426
01:17:36,230 --> 01:17:39,830
which kind of imprints this example into your weight vector.

1427
01:17:39,830 --> 01:17:43,550
So- and you can formally show that it actually increases your, uh,

1428
01:17:43,550 --> 01:17:45,560
margin after you do this.

1429
01:17:45,560 --> 01:17:49,595
Okay? Yeah?

1430
01:17:49,595 --> 01:17:52,085
What's the significance of the margin being 1?

1431
01:17:52,085 --> 01:17:55,025
What's the significance of the margin being 1?

1432
01:17:55,025 --> 01:17:57,050
Um, this is a little bit arbitrary,

1433
01:17:57,050 --> 01:18:00,440
you're just kind of sending a non-zero value.

1434
01:18:00,440 --> 01:18:02,795
Um, and, and, you know,

1435
01:18:02,795 --> 01:18:05,330
in support vector machines, you set it to 1,

1436
01:18:05,330 --> 01:18:07,850
and then you have regularization on the weights and

1437
01:18:07,850 --> 01:18:10,715
that gives you, uh, some interpretation.

1438
01:18:10,715 --> 01:18:12,740
So I don't have time to go over that right now,

1439
01:18:12,740 --> 01:18:15,830
but, uh, feel free to ask me later.

1440
01:18:15,830 --> 01:18:18,305
There's another loss function.

1441
01:18:18,305 --> 01:18:20,465
Uh, do you have a question?

1442
01:18:20,465 --> 01:18:23,990
Yeah. Why is the or why do we choose the margin if it's

1443
01:18:23,990 --> 01:18:28,025
a loss function that's supposed on the square or another loop?

1444
01:18:28,025 --> 01:18:29,510
Yeah. So why do you choose the margin?

1445
01:18:29,510 --> 01:18:32,300
So in classification, we're gonna look

1446
01:18:32,300 --> 01:18:35,630
at the margin because that tells you how comfortable when you're predicting,

1447
01:18:35,630 --> 01:18:37,505
uh, co- you know, correctly.

1448
01:18:37,505 --> 01:18:40,820
In regression, you're gonna look at residuals and square losses.

1449
01:18:40,820 --> 01:18:43,535
So it depends on what kind of- what problem you're trying to solve.

1450
01:18:43,535 --> 01:18:46,009
Um, just really quickly,

1451
01:18:46,009 --> 01:18:48,710
some of you might have heard of logistic regression.

1452
01:18:48,710 --> 01:18:51,380
Logistic regression is this, uh,

1453
01:18:51,380 --> 01:18:54,440
yellow loss function, right?

1454
01:18:54,440 --> 01:18:59,600
So the point of this is saying that this loss minimization framework is, you know,

1455
01:18:59,600 --> 01:19:02,090
really general and a lot of things that you might have heard of

1456
01:19:02,090 --> 01:19:05,270
least squares logistic regression are a kind of a special case of this.

1457
01:19:05,270 --> 01:19:08,435
So if you kind of master how to do loss minimization,

1458
01:19:08,435 --> 01:19:11,165
you kind of, uh, can do it all.

1459
01:19:11,165 --> 01:19:13,550
Okay. So summary, um,

1460
01:19:13,550 --> 01:19:15,830
basically, what's on the board here?

1461
01:19:15,830 --> 01:19:17,870
If you're doing classification,

1462
01:19:17,870 --> 01:19:21,440
you take the score which comes from the, uh,

1463
01:19:21,440 --> 01:19:24,860
w dot phi of x and you drive it into the sign,

1464
01:19:24,860 --> 01:19:26,630
and then you get either plus 1 or minus 1.

1465
01:19:26,630 --> 01:19:28,475
Regression, you just use a score.

1466
01:19:28,475 --> 01:19:32,465
Now to train, you have to assess how well you're doing.

1467
01:19:32,465 --> 01:19:34,955
In classification, there's a notion of a margin.

1468
01:19:34,955 --> 01:19:36,650
Res- uh, in regression,

1469
01:19:36,650 --> 01:19:39,800
it's the residual, and then you can define loss functions.

1470
01:19:39,800 --> 01:19:43,820
And here is we only talking about five loss functions but there's many others, um,

1471
01:19:43,820 --> 01:19:46,430
especially for a kind of structure prediction or ranking problems,

1472
01:19:46,430 --> 01:19:48,110
there's all sorts of different loss functions.

1473
01:19:48,110 --> 01:19:51,440
But they're kind of based on these simple ideas of,

1474
01:19:51,440 --> 01:19:52,550
you know, you have a hinge,

1475
01:19:52,550 --> 01:19:55,550
the upper balance is zero-one if you're doing classification and,

1476
01:19:55,550 --> 01:20:00,415
[NOISE] um, some sort of square-like error for, you know, regression.

1477
01:20:00,415 --> 01:20:03,805
And then, once you have your loss function, provided it's not zero-one,

1478
01:20:03,805 --> 01:20:06,430
you can optimize it using, um, SGD,

1479
01:20:06,430 --> 01:20:09,765
which turns out to be a lot faster than, you know, gradient descent.

1480
01:20:09,765 --> 01:20:13,055
Okay. So next time, we're gonna talk about, uh,

1481
01:20:13,055 --> 01:20:15,620
Phi of x, which we've kind of left as,

1482
01:20:15,620 --> 01:20:17,450
you know, someone just hands it to you.

1483
01:20:17,450 --> 01:20:19,490
And then we're also gonna talk about what is

1484
01:20:19,490 --> 01:20:21,785
the really true objective of machine learning?

1485
01:20:21,785 --> 01:20:24,500
Is it really to optimize the training loss?

1486
01:20:24,500 --> 01:20:27,510
Okay, until next time.